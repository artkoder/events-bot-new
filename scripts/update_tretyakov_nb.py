"""
Update Tretyakov parser in Kaggle notebook with PROVEN WORKING algorithm.
Tested locally with 100% success on all fields.
"""
import json

NOTEBOOK_PATH = "/workspaces/events-bot-new/kaggle/ParseTheatres/parse_theatres.ipynb"

# PROVEN WORKING CODE from local testing
NEW_TRETYAKOV_CODE = r'''
# ==========================================
# –ß–ê–°–¢–¨ 3: –¢–†–ï–¢–¨–Ø–ö–û–í–°–ö–ê–Ø –ì–ê–õ–ï–†–ï–Ø
# ==========================================

BASE_URL_TRETYAKOV = "https://kaliningrad.tretyakovgallery.ru"
MAX_EVENTS_TO_PROCESS = 1000  # Production limit
MAX_DATE_TIME_COMBOS = 30

MONTHS_RU = {
    "—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4, "–º–∞—è": 5, "–∏—é–Ω—è": 6,
    "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8, "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12
}


async def scrape_tretyakov_events_list(page):
    """Scrape events from /events/ page, extracting detail_url and ticket_url."""
    url = f"{BASE_URL_TRETYAKOV}/events/"
    print(f"\nüñºÔ∏è [–¢—Ä–µ—Ç—å—è–∫–æ–≤–∫–∞] Scanning: {url}")
    
    await page.goto(url, timeout=60000, wait_until='domcontentloaded')
    await page.wait_for_timeout(3000)
    
    for _ in range(3):
        await page.mouse.wheel(0, 3000)
        await page.wait_for_timeout(random.randint(1000, 1500))
    
    events = await page.evaluate("""
        () => {
            const events = [];
            const seen = new Set();
            const BASE = 'https://kaliningrad.tretyakovgallery.ru';
            
            document.querySelectorAll('.card').forEach(card => {
                const titleEl = card.querySelector('.card_title');
                if (!titleEl) return;
                
                const title = titleEl.innerText.trim();
                if (title.toUpperCase().includes('–≠–ö–°–ö–£–†–°–ò–Ø')) return;
                
                // Get detail_url from onclick
                let detailUrl = null;
                const onclick = card.getAttribute('onclick');
                if (onclick) {
                    const match = onclick.match(/window\\.open\\(['"]([^'"]+)['"]/);
                    if (match) detailUrl = match[1];
                }
                
                // Get ticket_url
                let ticketUrl = null;
                const ticketLink = card.querySelector('a[href*="tickets"]');
                if (ticketLink) {
                    let href = ticketLink.getAttribute('href');
                    if (href.startsWith('//')) ticketUrl = 'https:' + href;
                    else ticketUrl = href;
                }
                
                if (ticketUrl && ticketUrl.includes('timepad')) return;
                
                // Get photo
                let photo = null;
                const img = card.querySelector('img.card_img');
                if (img && img.src) {
                    photo = img.src.startsWith('/') ? BASE + img.src : img.src;
                }
                
                // Get location
                const text = card.innerText.toUpperCase();
                let location = '–¢—Ä–µ—Ç—å—è–∫–æ–≤–∫–∞ –ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥';
                if (text.includes('–ê–¢–†–ò–£–ú')) location = '–ê—Ç—Ä–∏—É–º';
                else if (text.includes('–ö–ò–ù–û–ó–ê–õ')) location = '–ö–∏–Ω–æ–∑–∞–ª';
                
                const key = title + ticketUrl;
                if (seen.has(key)) return;
                seen.add(key);
                
                if (ticketUrl) {
                    events.push({
                        title_raw: title,
                        detail_url: detailUrl,
                        ticket_url: ticketUrl,
                        photo: photo,
                        location: location
                    });
                }
            });
            return events;
        }
    """)
    
    print(f"   ‚úÖ Found {len(events)} events")
    return events[:MAX_EVENTS_TO_PROCESS]


async def scrape_tretyakov_detail(page, detail_url):
    """Visit detail page for title, description, AND date/time.
    
    CRITICAL: For Pianissimo festival events, each performer has their own
    detail page with the correct date (e.g. "6 —Ñ–µ–≤—Ä–∞–ª—è –≤ 20:00"). The shared
    ticket widget shows all festival dates, which caused phantom events.
    """
    import re
    import datetime
    
    if not detail_url:
        return {"title": None, "description": None, "parsed_date": None, "parsed_time": None}
    
    full_url = f"{BASE_URL_TRETYAKOV}{detail_url}" if detail_url.startswith('/') else detail_url
    print(f"   üìÑ Detail: {full_url}")
    
    try:
        await page.goto(full_url, timeout=30000, wait_until='domcontentloaded')
        await page.wait_for_timeout(2000)
        
        title = None
        h1 = await page.query_selector('h1')
        if h1:
            title = (await h1.inner_text()).strip()
        
        description = None
        paragraphs = await page.query_selector_all('p')
        for p in paragraphs:
            text = (await p.inner_text()).strip()
            if len(text) > 80 and not any(skip in text.lower() for skip in ['cookie', '–ø–æ–ª–∏—Ç–∏–∫', 'hours']):
                description = text[:500]
                break
        
        # Extract date and time from page text
        # Pattern: "6 —Ñ–µ–≤—Ä–∞–ª—è –≤ 20:00" or "6 —Ñ–µ–≤—Ä–∞–ª—è, –≤ 20:00"
        body_text = await page.inner_text("body")
        parsed_date = None
        parsed_time = None
        today = datetime.date.today()
        fallback = None
        
        for match in re.finditer(r'(\d{1,2})\s+([–∞-—è—ë]+)\s*,?\s*(?:–≤|–í)\s*(\d{1,2}:\d{2})', body_text, re.IGNORECASE):
            day = int(match.group(1))
            month_name = match.group(2).lower().strip('.,')
            time_str = match.group(3)
            
            month_num = MONTHS_RU.get(month_name)
            if not month_num:
                continue
            
            year = today.year
            # Handle year rollover
            if today.month >= 10 and month_num < 3:
                year += 1
            
            try:
                date_obj = datetime.date(year, month_num, day)
            except ValueError:
                continue
            
            if date_obj >= today:
                parsed_date = date_obj.isoformat()
                parsed_time = time_str
                break
            
            if fallback is None:
                fallback = (date_obj, time_str)
        
        if not parsed_date and fallback:
            parsed_date = fallback[0].isoformat()
            parsed_time = fallback[1]
        
        if parsed_date and parsed_time:
            print(f"      üìÖ Detail page date: {parsed_date} {parsed_time}")
        
        return {"title": title, "description": description, "parsed_date": parsed_date, "parsed_time": parsed_time}
    except Exception as e:
        print(f"      ‚ö†Ô∏è Detail error: {e}")
        return {"title": None, "description": None, "parsed_date": None, "parsed_time": None}


async def scrape_tretyakov_tickets(page, ticket_url):
    """Parse ticket page for dates, times, prices. Only clicks ACTIVE dates."""
    import datetime
    
    full_url = f"{BASE_URL_TRETYAKOV}{ticket_url}" if ticket_url.startswith('/') else ticket_url
    print(f"   üé´ Tickets: {full_url[:60]}...")
    today = datetime.date.today()
    results = []
    
    try:
        await page.goto(full_url, timeout=60000, wait_until='networkidle')
        await page.wait_for_timeout(3000)
        
        # Get ACTIVE dates only
        active_dates = await page.evaluate("""
            () => {
                const items = [];
                document.querySelectorAll('div.item.active').forEach(item => {
                    const dayEl = item.querySelector('.calendarDay');
                    const monthEl = item.querySelector('.calendarMonth');
                    if (dayEl) {
                        items.push({ 
                            day: dayEl.innerText.trim(), 
                            month: monthEl ? monthEl.innerText.trim().toLowerCase() : '' 
                        });
                    }
                });
                return items;
            }
        """)
        
        print(f"      üìÖ ACTIVE dates: {len(active_dates)}")
        
        for d in active_dates:
            if len(results) >= MAX_DATE_TIME_COMBOS:
                break
            
            month_num = MONTHS_RU.get(d['month'], 1)
            year = today.year + (1 if today.month >= 10 and month_num < 3 else 0)
            
            try:
                date_obj = datetime.date(year, month_num, int(d['day']))
                if date_obj < today:
                    continue
            except:
                continue
            
            date_iso = date_obj.isoformat()
            date_raw = f"{d['day']} {d['month']}"
            
            # Click date
            day_val = d['day']
            await page.evaluate(f"() => {{ document.querySelectorAll('div.item.active').forEach(i => {{ const dayEl = i.querySelector('.calendarDay'); if (dayEl && dayEl.innerText.trim() === '{day_val}') i.click(); }}); }}")
            await page.wait_for_timeout(1500)
            
            # Get times
            times = await page.evaluate("""() => [...document.querySelectorAll('label.select-time-button:not(.disabled)')].map(b => b.innerText.trim().match(/^\\d{1,2}:\\d{2}$/)?.[0]).filter(Boolean)""")
            
            if not times:
                times = ['00:00']
            
            print(f"         {date_raw}: {len(times)} times")
            
            for time_str in times:
                if len(results) >= MAX_DATE_TIME_COMBOS:
                    break
                
                # Click time
                await page.evaluate(f"() => {{ document.querySelectorAll('label.select-time-button').forEach(b => {{ if (b.innerText.includes('{time_str}')) b.click(); }}); }}")
                await page.wait_for_timeout(1500)
                
                # Click sector if present
                sectors = await page.locator('text=/[–°—Å]–µ–∫—Ç–æ—Ä/').all()
                if sectors:
                    try:
                        await sectors[0].click()
                        await page.wait_for_timeout(1000)
                    except:
                        pass
                
                # Extract price
                prices = await page.evaluate("""() => [...new Set([...document.querySelectorAll('*')].map(e => e.innerText?.match(/(\\d+)\\s*‚ÇΩ/)?.[1]).filter(Boolean).map(Number).filter(p => p > 100))]""")
                price = min(prices) if prices else None
                status = "available" if prices else "unknown"
                
                body = await page.inner_text("body")
                if "–≤—Å–µ –±–∏–ª–µ—Ç—ã –ø—Ä–æ–¥–∞–Ω—ã" in body.lower():
                    status = "sold_out"
                
                results.append({
                    "parsed_date": date_iso,
                    "parsed_time": time_str,
                    "date_raw": f"{date_raw} –≤ {time_str}",
                    "ticket_price_min": price,
                    "ticket_price_max": price,
                    "ticket_status": status,
                })
        
        return results
    except Exception as e:
        print(f"      ‚ö†Ô∏è Ticket error: {e}")
        return []


async def run_tretyakov(browser):
    """Main parser with proven working algorithm."""
    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})
    await context.route("**/*{google,yandex,metrika,analytics}*", lambda route: route.abort())
    
    list_page = await context.new_page()
    detail_page = await context.new_page()
    ticket_page = await context.new_page()

    events_raw = await scrape_tretyakov_events_list(list_page)
    if not events_raw:
        await context.close()
        return []

    all_events = []
    
    for idx, event in enumerate(events_raw):
        print(f"\nüìå [{idx+1}/{len(events_raw)}] {event['title_raw'][:50]}...")
        
        # Clean ticket_url
        raw_url = event['ticket_url']
        # 1. Remove absolute prefix if present
        if raw_url.startswith(BASE_URL_TRETYAKOV):
            raw_url = raw_url[len(BASE_URL_TRETYAKOV):]
        elif raw_url.startswith('http'):
            # External or other domain, keep as is but careful with base concatenation
            pass
            
        # 2. Remove trailing date/time components (e.g. /2026-01-07/20:00:00)
        # Regex for /YYYY-MM-DD/HH:MM:SS or /YYYY-MM-DD/HH:MM
        import re
        clean_url = re.sub(r'/\d{4}-\d{2}-\d{2}/\d{2}:\d{2}(:\d{2})?$', '', raw_url)
        clean_url = re.sub(r'/\d{4}-\d{2}-\d{2}/\d{2}:\d{2}(:\d{2})?$', '', clean_url) # Safety repeat
        
        # Use cleaned URL for processing
        print(f"   üîó Url: {clean_url}")
        
        # Get title, description, AND date from detail page
        detail = await scrape_tretyakov_detail(detail_page, event.get('detail_url'))
        title = detail['title'] or event['title_raw']
        description = detail['description']
        detail_date = detail.get('parsed_date')
        detail_time = detail.get('parsed_time')
        
        photo = event['photo']
        if photo and photo.startswith('/'):
            photo = f"{BASE_URL_TRETYAKOV}{photo}"
        
        # CRITICAL FIX: If detail page has date, USE IT as authoritative source
        # This fixes Pianissimo bug where shared ticket widget showed wrong dates
        if detail_date and detail_time:
            print(f"      ‚úÖ Using detail page date: {detail_date} {detail_time}")
            
            # Construct URL with detail page date
            base = clean_url
            if base.startswith('/'):
                base = f"{BASE_URL_TRETYAKOV}{base}"
            direct_url = f"{base}/{detail_date}/{detail_time}:00"
            
            # Get price from ticket widget for this specific date
            # (optional - we can try to verify if this date exists in widget)
            price = None
            status = "unknown"
            try:
                entries = await scrape_tretyakov_tickets(ticket_page, clean_url)
                def normalize_time(value):
                    if not value:
                        return value
                    parts = value.split(':')
                    if len(parts) != 2:
                        return value
                    try:
                        return f"{int(parts[0]):02d}:{int(parts[1]):02d}"
                    except ValueError:
                        return value
                
                target_time = normalize_time(detail_time)
                for e in entries:
                    if e['parsed_date'] == detail_date and normalize_time(e['parsed_time']) == target_time:
                        price = e['ticket_price_min']
                        status = e['ticket_status']
                        break
            except:
                pass
            
            # Format date_raw
            day = int(detail_date.split('-')[2])
            month_num = int(detail_date.split('-')[1])
            month_names = {1: '—è–Ω–≤–∞—Ä—è', 2: '—Ñ–µ–≤—Ä–∞–ª—è', 3: '–º–∞—Ä—Ç–∞', 4: '–∞–ø—Ä–µ–ª—è', 5: '–º–∞—è', 6: '–∏—é–Ω—è',
                          7: '–∏—é–ª—è', 8: '–∞–≤–≥—É—Å—Ç–∞', 9: '—Å–µ–Ω—Ç—è–±—Ä—è', 10: '–æ–∫—Ç—è–±—Ä—è', 11: '–Ω–æ—è–±—Ä—è', 12: '–¥–µ–∫–∞–±—Ä—è'}
            date_raw = f"{day} {month_names.get(month_num, '')} –≤ {detail_time}"
            
            all_events.append({
                "title": title,
                "description": description,
                "date_raw": date_raw,
                "parsed_date": detail_date,
                "parsed_time": detail_time,
                "ticket_status": status,
                "ticket_price_min": price,
                "ticket_price_max": price,
                "url": direct_url,
                "photos": [photo] if photo else [],
                "location": event['location'],
                "scene": event['location'] if event['location'] in ["–ê—Ç—Ä–∏—É–º", "–ö–∏–Ω–æ–∑–∞–ª"] else ""
            })
        else:
            # No detail page date - use ticket widget dates (original behavior)
            entries = await scrape_tretyakov_tickets(ticket_page, clean_url)
            
            if not entries:
                target_url = clean_url
                if target_url.startswith('/'):
                    target_url = f"{BASE_URL_TRETYAKOV}{target_url}"
                    
                all_events.append({
                    "title": title,
                    "description": description,
                    "date_raw": "",
                    "parsed_date": None,
                    "parsed_time": None,
                    "ticket_status": "unknown",
                    "ticket_price_min": None,
                    "ticket_price_max": None,
                    "url": target_url,
                    "photos": [photo] if photo else [],
                    "location": event['location'],
                    "scene": event['location'] if event['location'] in ["–ê—Ç—Ä–∏—É–º", "–ö–∏–Ω–æ–∑–∞–ª"] else ""
                })
            else:
                for e in entries:
                    # Construct clean direct URL
                    base = clean_url
                    if base.startswith('/'):
                        base = f"{BASE_URL_TRETYAKOV}{base}"
                    
                    direct_url = f"{base}/{e['parsed_date']}/{e['parsed_time']}:00"
                    all_events.append({
                        "title": title,
                        "description": description,
                        "date_raw": e['date_raw'],
                        "parsed_date": e['parsed_date'],
                        "parsed_time": e['parsed_time'],
                        "ticket_status": e['ticket_status'],
                        "ticket_price_min": e['ticket_price_min'],
                        "ticket_price_max": e['ticket_price_max'],
                        "url": direct_url,
                        "photos": [photo] if photo else [],
                        "location": event['location'],
                        "scene": event['location'] if event['location'] in ["–ê—Ç—Ä–∏—É–º", "–ö–∏–Ω–æ–∑–∞–ª"] else ""
                    })

    print(f"\nüéâ [–¢—Ä–µ—Ç—å—è–∫–æ–≤–∫–∞] Total: {len(all_events)} event entries")
    await context.close()
    return all_events
'''


def update_notebook():
    with open(NOTEBOOK_PATH, 'r', encoding='utf-8') as f:
        nb = json.load(f)

    for cell in nb.get("cells", []):
        if cell.get("cell_type") == "code":
            source = "".join(cell["source"])
            start = "# ==========================================\n# –ß–ê–°–¢–¨ 3: –¢–†–ï–¢–¨–Ø–ö–û–í–°–ö–ê–Ø –ì–ê–õ–ï–†–ï–Ø\n# =========================================="
            end = "# --- –ó–ê–ü–£–°–ö ---"
            
            if start in source and end in source:
                parts = source.split(start)
                pre = parts[0]
                rest = parts[1].split(end)[1]
                
                new = pre + NEW_TRETYAKOV_CODE + "\n\n" + end + rest
                cell["source"] = [line + '\n' for line in new.split('\n')]
                if cell["source"][-1] == '\n':
                    cell["source"].pop()
                else:
                    cell["source"][-1] = cell["source"][-1].rstrip('\n')
                
                print("‚úÖ Notebook updated with PROVEN WORKING algorithm")
                print("   - detail_url from onclick for title+description")
                print("   - ACTIVE dates only (div.item.active)")
                print("   - Sector handling for prices")
                break
    
    with open(NOTEBOOK_PATH, 'w', encoding='utf-8') as f:
        json.dump(nb, f, indent=1, ensure_ascii=False)


if __name__ == "__main__":
    update_notebook()
