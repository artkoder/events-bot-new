"""
Kaggle notebook –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–æ–±—ã—Ç–∏–π —Å —Å–∞–π—Ç–∞ pyramida.info.

–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: —Å–ø–∏—Å–æ–∫ URL –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è PYRAMIDA_URLS.
–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: pyramida_events.json —Å —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–º–∏ —Å–æ–±—ã—Ç–∏—è–º–∏.
"""

import asyncio
import os
import subprocess
import sys
import pandas as pd
import re
import json

# --- 1. –£–°–¢–ê–ù–û–í–ö–ê ---
def install_libs():
    try:
        import playwright
        import bs4
    except ImportError:
        print("‚è≥ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "playwright", "beautifulsoup4", "pandas"])
        os.system("playwright install chromium")
        os.system("playwright install-deps")
        print("‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≥–æ—Ç–æ–≤—ã.")

install_libs()

from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

BASE_DOMAIN = "https://pyramida.info"

# --- 2. –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---
def clean_text(text):
    if not text: return ""
    return re.sub(r'\s+', ' ', text).strip()

# --- 3. –ü–ê–†–°–ï–† ---
async def parse_pyramida_event(page, url):
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞: {url}")
    
    try:
        # –ñ–¥–µ–º —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Ç–∏ (networkidle) –¥–ª—è SPA —Å–∞–π—Ç–æ–≤
        await page.goto(url, timeout=60000, wait_until='networkidle')
        
        # –ü–∞—É–∑–∞ –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ React
        await page.wait_for_timeout(3000)

        # –ñ–¥–µ–º H1 (–≥–∞—Ä–∞–Ω—Ç–∏—è, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ—è–≤–∏–ª—Å—è)
        try:
            await page.wait_for_selector('h1', state='visible', timeout=10000)
        except:
            print("‚ö†Ô∏è H1 –Ω–µ –ø–æ—è–≤–∏–ª—Å—è —Å—Ä–∞–∑—É, –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å...")

        content = await page.content()
        soup = BeautifulSoup(content, 'html.parser')
        
        # --- –ó–ê–ì–û–õ–û–í–û–ö ---
        h1 = soup.find('h1')
        title = clean_text(h1.get_text()) if h1 else "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω"
        
        if title == "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω":
            return None

        # --- –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï ---
        image_url = ""
        # 1. –ü–æ alt
        img = soup.find('img', alt=title)
        if img: image_url = img.get('src')
        # 2. –ü–æ –ø—É—Ç–∏ upload
        if not image_url:
            for i in soup.find_all('img'):
                src = i.get('src', '')
                if '/upload/' in src and 'resize_cache' in src:
                    image_url = src
                    break
        
        if image_url and image_url.startswith('/'):
            image_url = f"{BASE_DOMAIN}{image_url}"

        # --- –í–û–ó–†–ê–°–¢ ---
        age = ""
        for div in soup.find_all(['div', 'span']):
            txt = div.get_text(" ", strip=True)
            # –ò—â–µ–º –∫–æ—Ä–æ—Ç–∫—É—é —Å—Ç—Ä–æ–∫—É "12 +" –∏–ª–∏ "0+"
            if len(txt) < 6 and re.match(r'^\d+\s*\+$', txt):
                age = txt.replace(" ", "")
                break

        # --- –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–û–ò–°–ö –ü–û–õ–ï–ô (–î–ê–¢–ê, –ú–ï–°–¢–û, –¶–ï–ù–ê) ---
        def get_value_from_row(soup, label_pattern):
            label_tag = soup.find(string=re.compile(label_pattern))
            if not label_tag: return ""
            
            current = label_tag.parent
            for _ in range(3):
                if not current: break
                row_text = clean_text(current.get_text(" "))
                clean_label = label_tag.strip()
                # –ï—Å–ª–∏ –≤ —Å—Ç—Ä–æ–∫–µ —Ç–µ–∫—Å—Ç–∞ –±–æ–ª—å—à–µ, —á–µ–º –≤ –ª–µ–π–±–ª–µ - –∑–Ω–∞—á–∏—Ç —Ç–∞–º –∑–Ω–∞—á–µ–Ω–∏–µ
                if len(row_text) > len(clean_label) + 2:
                    val = re.sub(fr'{label_pattern}[:\s]*', '', row_text, count=1, flags=re.I).strip()
                    if "Previous" in val or "Next" in val: continue 
                    return val
                current = current.parent
            return ""

        # –ú–ï–°–¢–û
        location = get_value_from_row(soup, "–ú–µ—Å—Ç–æ")
        
        # –¶–ï–ù–ê
        price = get_value_from_row(soup, "–¶–µ–Ω–∞")
        if not price and ("–í—Ö–æ–¥ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π" in soup.get_text() or "–≤—Ö–æ–¥ —Å–≤–æ–±–æ–¥–Ω—ã–π" in soup.get_text().lower()):
            price = "–ë–µ—Å–ø–ª–∞—Ç–Ω–æ"
        
        # –î–ê–¢–ê
        date_raw = get_value_from_row(soup, "–î–∞—Ç–∞")
        # –§–æ–ª–±–µ–∫ 1: input
        if not date_raw:
            label_date = soup.find(string=re.compile("–î–∞—Ç–∞"))
            if label_date and label_date.parent:
                parent = label_date.parent
                inp = parent.find_next('input')
                if inp: date_raw = inp.get('value', '')

        # –§–æ–ª–±–µ–∫ 2: –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ (–¥–ª—è —Å–æ–±—ã—Ç–∏–π –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –¥–∞—Ç—ã –≤ —à–∞–ø–∫–µ)
        if not date_raw:
            schedule_header = soup.find(string=re.compile(r"–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ[:\.]?", re.I))
            if schedule_header and schedule_header.parent:
                parent = schedule_header.parent
                full_text = parent.get_text("\n", strip=True)
                # –ï—Å–ª–∏ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞, –±–µ—Ä–µ–º —Å–æ—Å–µ–¥–µ–π
                if len(full_text) < 20:
                    siblings = parent.find_next_siblings()
                    full_text += "\n" + "\n".join([s.get_text("\n", strip=True) for s in siblings[:3]])
                
                dates = []
                for line in full_text.split('\n'):
                    if re.search(r'\d{1,2}\s+[–∞-—è–ê-–Ø—ë–Å]+', line) and re.search(r'\d{2}:\d{2}', line):
                        dates.append(line.strip())
                if dates: date_raw = " | ".join(dates)

        # --- –°–¢–ê–¢–£–° ---
        status = "unknown"
        page_text = soup.get_text(" ", strip=True).lower()
        if soup.find('button', string=re.compile(r"–ö—É–ø–∏—Ç—å", re.I)):
            status = "available"
        elif "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü" in page_text and ("–ø—Ä–æ–π—Ç–∏" in page_text or "–Ω–µ–æ–±—Ö–æ–¥–∏–º–∞" in page_text):
             status = "registration_open"
        elif "–±–∏–ª–µ—Ç–æ–≤ –Ω–µ—Ç" in page_text:
            status = "sold_out"

        # --- –û–ü–ò–°–ê–ù–ò–ï ---
        description = ""
        desc_label = soup.find(string=re.compile("^–û–ø–∏—Å–∞–Ω–∏–µ"))
        if desc_label and desc_label.parent:
            description = desc_label.parent.get_text("\n", strip=True).replace("–û–ø–∏—Å–∞–Ω–∏–µ:", "").strip()
            for sw in ["–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û–ë –û–†–ì–ê–ù–ò–ó–ê–¢–û–†–ï", "–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:", "–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ:"]:
                if sw in description: description = description.split(sw)[0].strip()

        return {
            "title": title,
            "date_raw": date_raw,
            "location": location,
            "price": price,
            "age_restriction": age,
            "ticket_status": status,
            "url": url,            # <--- –í–ê–ñ–ù–û: –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–∫—É–ø–∫—É (–∏—Å—Ö–æ–¥–Ω–∞—è —Å—Å—ã–ª–∫–∞)
            "image_url": image_url,
            "description": description[:1000]  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª–∏–Ω—É –æ–ø–∏—Å–∞–Ω–∏—è
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ {url}: {e}")
        return None

# --- 4. –ó–ê–ü–£–°–ö ---
async def main():
    # –°–ü–ò–°–û–ö –°–°–´–õ–û–ö (–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï)
    # –ü–æ–ª—É—á–∞–µ–º URLs –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ
    urls_env = os.environ.get("PYRAMIDA_URLS", "")
    if urls_env:
        urls = [u.strip() for u in urls_env.split(",") if u.strip()]
    else:
        # –¢–µ—Å—Ç–æ–≤—ã–µ URLs –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
        urls = [
            "https://pyramida.info/tickets/novogodnee-nastroenie-ot-tantsy_54151730",
            "https://pyramida.info/tickets/minecraft-shou_49516085",
            "https://pyramida.info/tickets/puteshestvie-v-skazochnyy-son-vtoraya-partiya-biletov_53469631"
        ]
    
    print(f"üìã –ü–∞—Ä—Å–∏–Ω–≥ {len(urls)} URL(s)...")
    results = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(viewport={'width': 1920, 'height': 1080})
        page = await context.new_page()

        for url in urls:
            data = await parse_pyramida_event(page, url)
            if data:
                results.append(data)
        
        await browser.close()

    if results:
        df = pd.DataFrame(results)
        print("\nüéâ –ì–æ—Ç–æ–≤–æ! –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
        # –í—ã–≤–æ–¥ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è
        print(df[['title', 'date_raw', 'price', 'url']].to_string())
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        df.to_json('pyramida_events.json', orient='records', force_ascii=False, indent=4)
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ pyramida_events.json")
    else:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Å—Ç–æ–π JSON –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
        with open('pyramida_events.json', 'w', encoding='utf-8') as f:
            json.dump([], f)
        print("‚ö†Ô∏è –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, —Å–æ–∑–¥–∞–Ω –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª.")

# –ó–∞–ø—É—Å–∫
asyncio.get_event_loop().run_until_complete(main())
