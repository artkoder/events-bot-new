{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "# --- 1. –£–°–¢–ê–ù–û–í–ö–ê –ë–ò–ë–õ–ò–û–¢–ï–ö ---\n",
    "def install_libs():\n",
    "    try:\n",
    "        import playwright\n",
    "        import bs4\n",
    "    except ImportError:\n",
    "        print(\"‚è≥ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"playwright\", \"beautifulsoup4\", \"pandas\"])\n",
    "        os.system(\"playwright install chromium\")\n",
    "        os.system(\"playwright install-deps\")\n",
    "        print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≥–æ—Ç–æ–≤—ã.\")\n",
    "\n",
    "install_libs()\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ==========================================\n",
    "# –ß–ê–°–¢–¨ 1: –î–†–ê–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –¢–ï–ê–¢–† (DRAMTEATR)\n",
    "# ==========================================\n",
    "\n",
    "async def scrape_dram_schedule(page):\n",
    "    url = \"https://dramteatr39.ru/afisha\"\n",
    "    print(f\"\\nüé≠ [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –≠—Ç–∞–ø 1: –°–∫–∞–Ω–∏—Ä—É–µ–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ {url}...\")\n",
    "\n",
    "    try:\n",
    "        await page.goto(url, timeout=90000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(3000)\n",
    "\n",
    "        # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞\n",
    "        scrolls = 6\n",
    "        for _ in range(scrolls):\n",
    "            await page.mouse.wheel(0, 4000)\n",
    "            await page.wait_for_timeout(random.randint(1000, 3000))\n",
    "\n",
    "        content = await page.content()\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        events_list = []\n",
    "        event_links = soup.find_all('a', href=re.compile(r'/spektakli/'))\n",
    "        seen_identifiers = set()\n",
    "\n",
    "        for link in event_links:\n",
    "            href = link.get('href')\n",
    "            full_link = f\"https://dramteatr39.ru{href}\" if href.startswith('/') else href\n",
    "\n",
    "            title = link.get_text(strip=True)\n",
    "            if len(title) < 2:\n",
    "                continue\n",
    "\n",
    "            container = link.find_parent(class_='affiche__container')\n",
    "            card = container\n",
    "            date_block = None\n",
    "            if container:\n",
    "                date_block = container.select_one('.affiche__date-block')\n",
    "            else:\n",
    "                # Fallback for unexpected markup changes.\n",
    "                card = link.find_parent()\n",
    "                if card:\n",
    "                    date_block = card.select_one('.affiche__date-block')\n",
    "\n",
    "            if not card:\n",
    "                continue\n",
    "\n",
    "            card_text = card.get_text(\" \", strip=True).upper()\n",
    "\n",
    "            # –î–∞—Ç–∞\n",
    "            date_text = \"–î–∞—Ç–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\"\n",
    "            day_text = \"\"\n",
    "            month_text = \"\"\n",
    "            time_text = \"\"\n",
    "\n",
    "            if date_block:\n",
    "                day_node = date_block.select_one('.date-list__day')\n",
    "                month_node = date_block.select_one('.date-list__month')\n",
    "                time_node = date_block.select_one('.date-list__time')\n",
    "\n",
    "                if day_node:\n",
    "                    day_text = day_node.get_text(strip=True)\n",
    "                if month_node:\n",
    "                    month_text = month_node.get_text(strip=True)\n",
    "                if time_node:\n",
    "                    spans = [s.get_text(strip=True) for s in time_node.find_all('span') if s.get_text(strip=True)]\n",
    "                    if spans:\n",
    "                        time_text = spans[-1]\n",
    "                    else:\n",
    "                        raw_time = time_node.get_text(\" \", strip=True)\n",
    "                        parts = [p for p in raw_time.split() if \":\" in p]\n",
    "                        if parts:\n",
    "                            time_text = parts[-1]\n",
    "\n",
    "            if day_text and month_text:\n",
    "                date_text = f\"{day_text} {month_text}\"\n",
    "                if time_text:\n",
    "                    date_text = f\"{date_text} {time_text}\"\n",
    "\n",
    "            # –°—Ç–∞—Ç—É—Å\n",
    "            status = \"available\"\n",
    "            if \"–ë–ò–õ–ï–¢–û–í –ù–ï–¢\" in card_text or \"–ü–†–û–î–ê–ù–û\" in card_text:\n",
    "                status = \"sold_out\"\n",
    "\n",
    "            # –°—Ü–µ–Ω–∞\n",
    "            if \"–û–°–ù–û–í–ù–ê–Ø\" in card_text:\n",
    "                scene = \"–û—Å–Ω–æ–≤–Ω–∞—è\"\n",
    "            elif \"–°–†–ï–î–ù–Ø–Ø\" in card_text:\n",
    "                scene = \"–°—Ä–µ–¥–Ω—è—è\"\n",
    "            else:\n",
    "                scene = \"–ú–∞–ª–∞—è\"\n",
    "\n",
    "            event_id = f\"{date_text}_{title}\"\n",
    "            if event_id in seen_identifiers:\n",
    "                continue\n",
    "            seen_identifiers.add(event_id)\n",
    "\n",
    "            events_list.append({\n",
    "                \"title\": title,\n",
    "                \"date_raw\": date_text,\n",
    "                \"ticket_status\": status,\n",
    "                \"scene\": scene,\n",
    "                \"url\": full_link,\n",
    "                \"location\": \"–î—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ–∞—Ç—Ä\"\n",
    "            })\n",
    "\n",
    "        print(f\"‚úÖ [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events_list)}\")\n",
    "        return events_list\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è: {e}\")\n",
    "        return []\n",
    "\n",
    "async def scrape_dram_details(context, unique_urls):\n",
    "    print(f\"üïµÔ∏è‚Äç‚ôÇÔ∏è [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –°–±–æ—Ä –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {len(unique_urls)} —Å–ø–µ–∫—Ç–∞–∫–ª–µ–π...\")\n",
    "    details_map = {}\n",
    "    page = await context.new_page()\n",
    "\n",
    "    for url in unique_urls:\n",
    "        try:\n",
    "            await page.goto(url, timeout=45000, wait_until='domcontentloaded')\n",
    "            try:\n",
    "                await page.wait_for_selector('h1', timeout=3000)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            content = await page.content()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            full_text = soup.get_text(\" \", strip=True).upper()\n",
    "\n",
    "            is_pushkin = \"–ü–£–®–ö–ò–ù–°–ö–ê–Ø –ö–ê–†–¢–ê\" in full_text\n",
    "\n",
    "            images = []\n",
    "            for a in soup.find_all('a', href=re.compile(r'\\.(jpg|jpeg|png)$', re.IGNORECASE)):\n",
    "                img_url = a['href']\n",
    "                if img_url.startswith('/'):\n",
    "                    img_url = f\"https://dramteatr39.ru{img_url}\"\n",
    "                if img_url not in images:\n",
    "                    images.append(img_url)\n",
    "\n",
    "            if len(images) < 2:\n",
    "                for img in soup.select('div.swiper-slide img, .gallery img, .content img'):\n",
    "                    src = img.get('src')\n",
    "                    if src and 'logo' not in src:\n",
    "                        if src.startswith('/'):\n",
    "                            src = f\"https://dramteatr39.ru{src}\"\n",
    "                        if src not in images:\n",
    "                            images.append(src)\n",
    "\n",
    "            desc_text = \"\"\n",
    "            desc_div = soup.find('div', class_=re.compile(r'description|text-block'))\n",
    "            if not desc_div:\n",
    "                header = soup.find(string=re.compile(\"–û —Å–ø–µ–∫—Ç–∞–∫–ª–µ\"))\n",
    "                if header and header.parent:\n",
    "                    parent = header.find_parent('div')\n",
    "                    if parent:\n",
    "                        desc_div = parent\n",
    "            if desc_div:\n",
    "                desc_text = desc_div.get_text(\"\\n\", strip=True)\n",
    "\n",
    "            creators = \"\"\n",
    "            c_div = soup.find('div', class_=re.compile(r'creators|team'))\n",
    "            if c_div:\n",
    "                creators = c_div.get_text(\" | \", strip=True)\n",
    "\n",
    "            details_map[url] = {\n",
    "                \"pushkin_card\": is_pushkin,\n",
    "                \"photos\": images,\n",
    "                \"description\": desc_text[:1500],\n",
    "                \"creators\": creators[:500]\n",
    "            }\n",
    "        except Exception:\n",
    "            details_map[url] = {}\n",
    "\n",
    "    await page.close()\n",
    "    return details_map\n",
    "\n",
    "async def run_dramteatr(browser):\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    page = await context.new_page()\n",
    "\n",
    "    schedule = await scrape_dram_schedule(page)\n",
    "    if not schedule:\n",
    "        await context.close()\n",
    "        return []\n",
    "\n",
    "    unique_links = list(set(x['url'] for x in schedule))\n",
    "    details = await scrape_dram_details(context, unique_links)\n",
    "\n",
    "    await context.close()\n",
    "\n",
    "    final_data = []\n",
    "    for item in schedule:\n",
    "        det = details.get(item['url'], {})\n",
    "        final_data.append({**item, **det})\n",
    "\n",
    "    return final_data\n",
    "\n",
    "# ==========================================\n",
    "# –ß–ê–°–¢–¨ 2: –ú–£–ó–´–ö–ê–õ–¨–ù–´–ô –¢–ï–ê–¢–† (MUZTEATR)\n",
    "# ==========================================\n",
    "\n",
    "BASE_URL_MUZ = \"https://muzteatr39.ru\"\n",
    "\n",
    "async def find_muz_afisha_link(page):\n",
    "    try:\n",
    "        await page.goto(BASE_URL_MUZ, timeout=30000, wait_until='domcontentloaded')\n",
    "        link = page.get_by_role(\"link\", name=re.compile(\"–ê—Ñ–∏—à–∞\", re.IGNORECASE)).first\n",
    "        href = await link.get_attribute(\"href\")\n",
    "        if href:\n",
    "            return f\"{BASE_URL_MUZ}{href}\" if href.startswith(\"/\") else href\n",
    "    except:\n",
    "        pass\n",
    "    return f\"{BASE_URL_MUZ}/afisha\"\n",
    "\n",
    "async def scrape_muz_schedule(page, afisha_url):\n",
    "    print(f\"\\nüéª [–ú—É–∑—Ç–µ–∞—Ç—Ä] –≠—Ç–∞–ø 1: –°–∫–∞–Ω–∏—Ä—É–µ–º –∞—Ñ–∏—à—É: {afisha_url}...\")\n",
    "    try:\n",
    "        await page.goto(afisha_url, timeout=60000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(2000)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # –°–∫—Ä–æ–ª–ª\n",
    "    try:\n",
    "        for _ in range(3):\n",
    "            await page.mouse.wheel(0, 4000)\n",
    "            await page.wait_for_timeout(1000)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    content = await page.content()\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    events_list = []\n",
    "    unique_ids = set()\n",
    "    MONTHS_MAP = {'–Ø–ù–í–ê–†': '–Ø–ù–í–ê–†–Ø', '–§–ï–í–†–ê–õ': '–§–ï–í–†–ê–õ–Ø', '–ú–ê–†–¢': '–ú–ê–†–¢–ê', '–ê–ü–†–ï–õ': '–ê–ü–†–ï–õ–Ø', '–ú–ê–ô': '–ú–ê–Ø', '–ò–Æ–ù': '–ò–Æ–ù–Ø', '–ò–Æ–õ': '–ò–Æ–õ–Ø', '–ê–í–ì–£–°–¢': '–ê–í–ì–£–°–¢–ê', '–°–ï–ù–¢–Ø–ë–†': '–°–ï–ù–¢–Ø–ë–†–Ø', '–û–ö–¢–Ø–ë–†': '–û–ö–¢–Ø–ë–†–Ø', '–ù–û–Ø–ë–†': '–ù–û–Ø–ë–†–Ø', '–î–ï–ö–ê–ë–†': '–î–ï–ö–ê–ë–†–Ø'}\n",
    "    last_seen_month = \"?\"\n",
    "\n",
    "    rows = soup.find_all('div', class_=re.compile(r'row\\s+afisha'))\n",
    "    for row in rows:\n",
    "        date_col = row.find('div', class_=re.compile(r'afisha_data'))\n",
    "        day_str, month_str, time_str = \"?\", last_seen_month, \"00:00\"\n",
    "\n",
    "        if date_col:\n",
    "            col_text = date_col.get_text(\" \", strip=True).upper()\n",
    "            day_div = date_col.find('div', class_='afisha_nom')\n",
    "            if day_div:\n",
    "                day_str = day_div.get_text(strip=True)\n",
    "            else:\n",
    "                day_match = re.search(r'^(\\d{1,2})', col_text)\n",
    "                if day_match:\n",
    "                    day_str = day_match.group(1)\n",
    "\n",
    "            for root, full_name in MONTHS_MAP.items():\n",
    "                if root in col_text:\n",
    "                    month_str = full_name\n",
    "                    last_seen_month = full_name\n",
    "                    break\n",
    "\n",
    "            time_match = re.search(r'(\\d{1,2}:\\d{2})', col_text)\n",
    "            if time_match:\n",
    "                time_str = time_match.group(1).zfill(5)\n",
    "\n",
    "        if not day_str.isdigit():\n",
    "            continue\n",
    "        date_final = f\"{day_str} {month_str} {time_str}\"\n",
    "\n",
    "        title_div = row.find('div', class_='afisha_spek_title')\n",
    "        if not title_div:\n",
    "            continue\n",
    "\n",
    "        title = \"\"\n",
    "        link_tag = title_div.find('a')\n",
    "        if link_tag:\n",
    "            title = link_tag.get_text(strip=True)\n",
    "            href = link_tag.get('href')\n",
    "        else:\n",
    "            badge = title_div.find('span', class_='badge')\n",
    "            if badge:\n",
    "                badge.decompose()\n",
    "            title = title_div.get_text(strip=True)\n",
    "            href = None\n",
    "\n",
    "        if not href:\n",
    "            continue\n",
    "        full_link = f\"{BASE_URL_MUZ}{href}\" if href.startswith('/') else href\n",
    "\n",
    "        badge = row.find('span', class_='badge')\n",
    "        age = badge.get_text(strip=True) if badge else \"\"\n",
    "\n",
    "        row_text_full = row.get_text(\" \", strip=True).upper()\n",
    "        status = \"available\"\n",
    "        if \"–ë–ò–õ–ï–¢–û–í –ù–ï–¢\" in row_text_full or \"–ü–†–û–î–ê–ù–û\" in row_text_full:\n",
    "            status = \"sold_out\"\n",
    "\n",
    "        eid = f\"{date_final}_{full_link}\"\n",
    "        if eid in unique_ids:\n",
    "            continue\n",
    "        unique_ids.add(eid)\n",
    "\n",
    "        events_list.append({\n",
    "            \"title\": title,\n",
    "            \"date_raw\": date_final,\n",
    "            \"age_restriction\": age,\n",
    "            \"ticket_status\": status,\n",
    "            \"url\": full_link,\n",
    "            \"location\": \"–ú—É–∑—ã–∫–∞–ª—å–Ω—ã–π —Ç–µ–∞—Ç—Ä\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ [–ú—É–∑—Ç–µ–∞—Ç—Ä] –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events_list)}\")\n",
    "    return events_list\n",
    "\n",
    "async def scrape_muz_details(context, unique_urls):\n",
    "    print(f\"üïµÔ∏è‚Äç‚ôÇÔ∏è [–ú—É–∑—Ç–µ–∞—Ç—Ä] –°–±–æ—Ä –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {len(unique_urls)} —Å–æ–±—ã—Ç–∏–π...\")\n",
    "    details = {}\n",
    "    page = await context.new_page()\n",
    "    BAD_IMAGES = ['ulogin', 'provider', 'icon', 'logo', 'social', 'vk.com', 'ok.ru', 'blank', 'pixel']\n",
    "\n",
    "    for url in unique_urls:\n",
    "        try:\n",
    "            if not url or 'http' not in url:\n",
    "                continue\n",
    "            try:\n",
    "                await page.goto(url, timeout=30000, wait_until='domcontentloaded')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            content = await page.content()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            desc_text = \"\"\n",
    "            meta_og = soup.find(\"meta\", property=\"og:description\")\n",
    "            if meta_og and meta_og.get(\"content\"):\n",
    "                desc_text = meta_og[\"content\"].strip()\n",
    "            if not desc_text:\n",
    "                meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "                if meta_desc and meta_desc.get(\"content\"):\n",
    "                    desc_text = meta_desc[\"content\"].strip()\n",
    "            if not desc_text:\n",
    "                content_div = soup.find('div', class_=re.compile(r'detail-text|news-detail|item-text'))\n",
    "                if content_div:\n",
    "                    raw = content_div.get_text(\"\\n\", strip=True)\n",
    "                    lines = [l for l in raw.split('\\n') if len(l) > 30 and '–≤–æ–π—Ç–∏' not in l.lower()]\n",
    "                    desc_text = \"\\n\".join(lines)[:2000]\n",
    "\n",
    "            images = []\n",
    "            for d in soup.find_all('div', style=re.compile(r'url\\(')):\n",
    "                match = re.search(r'url\\([\\'\\\"]?(.*?)[\\'\\\"]?\\)', d['style'])\n",
    "                if match:\n",
    "                    u = match.group(1)\n",
    "                    if u.startswith('/'):\n",
    "                        u = f\"{BASE_URL_MUZ}{u}\"\n",
    "                    if not any(b in u.lower() for b in BAD_IMAGES):\n",
    "                        images.append(u)\n",
    "\n",
    "            container = soup.find('div', class_=re.compile(r'container|main|content')) or soup\n",
    "            for img in container.find_all('img'):\n",
    "                src = img.get('src')\n",
    "                if src and not src.endswith('.svg') and not any(b in src.lower() for b in BAD_IMAGES):\n",
    "                    if src.startswith('/'):\n",
    "                        src = f\"{BASE_URL_MUZ}{src}\"\n",
    "                    images.append(src)\n",
    "\n",
    "            details[url] = {\n",
    "                \"pushkin_card\": \"–ü–£–®–ö–ò–ù–°–ö–ê–Ø –ö–ê–†–¢–ê\" in soup.get_text().upper(),\n",
    "                \"description\": desc_text,\n",
    "                \"photos\": list(set(images))\n",
    "            }\n",
    "        except:\n",
    "            details[url] = {}\n",
    "\n",
    "    await page.close()\n",
    "    return details\n",
    "\n",
    "async def run_muzteatr(browser):\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    page = await context.new_page()\n",
    "\n",
    "    afisha_url = await find_muz_afisha_link(page)\n",
    "    events = await scrape_muz_schedule(page, afisha_url)\n",
    "\n",
    "    if not events:\n",
    "        await context.close()\n",
    "        return []\n",
    "\n",
    "    urls = list(set(x['url'] for x in events))\n",
    "    details = await scrape_muz_details(context, urls)\n",
    "\n",
    "    await context.close()\n",
    "\n",
    "    final = []\n",
    "    for ev in events:\n",
    "        det = details.get(ev['url'], {})\n",
    "        final.append({**ev, **det})\n",
    "    return final\n",
    "\n",
    "BASE_URL_SOBOR = \"https://sobor39.ru\"\n",
    "\n",
    "# --- 2. –ü–ê–†–°–ï–† –°–û–ë–û–†–ê ---\n",
    "async def scrape_sobor_strict(page):\n",
    "    target_url = \"https://sobor39.ru/events/concerts/night/\"\n",
    "    print(f\"‚õ™ –ó–∞—Ö–æ–¥–∏–º –Ω–∞ {target_url}...\")\n",
    "\n",
    "    try:\n",
    "        await page.goto(target_url, timeout=90000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(5000)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏: {e}. –ü—Ä–æ–±—É–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–µ–º, —á—Ç–æ –µ—Å—Ç—å.\")\n",
    "\n",
    "    # --- –ü–†–û–ö–†–£–¢–ö–ê ---\n",
    "    print(\"‚¨áÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∫—É –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ —Å–æ–±—ã—Ç–∏–π...\")\n",
    "\n",
    "    for i in range(10):\n",
    "        await page.mouse.wheel(0, 5000)\n",
    "        await page.wait_for_timeout(3000)\n",
    "\n",
    "        try:\n",
    "            more_btns = page.locator(\"text=/–ü–æ–∫–∞–∑–∞—Ç—å –µ—â[—ë–µ]|–ó–∞–≥—Ä—É–∑–∏—Ç—å/i\")\n",
    "            if await more_btns.count() > 0:\n",
    "                if await more_btns.first.is_visible():\n",
    "                    await more_btns.first.click()\n",
    "                    await page.wait_for_timeout(2000)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            print(f\"   ...–ø—Ä–æ–∫—Ä—É—Ç–∫–∞ {i+1}/10\")\n",
    "\n",
    "    content = await page.content()\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    events_list = []\n",
    "    unique_keys = set()\n",
    "\n",
    "    items = soup.find_all('div', class_='list-item')\n",
    "    print(f\"üîç –ù–∞–π–¥–µ–Ω–æ –±–ª–æ–∫–æ–≤ .list-item: {len(items)}\")\n",
    "\n",
    "    for item in items:\n",
    "        try:\n",
    "            # 1. –ó–ê–ì–û–õ–û–í–û–ö\n",
    "            title_tag = item.find('div', class_='list-t')\n",
    "            if not title_tag:\n",
    "                continue\n",
    "            title = title_tag.get_text(strip=True)\n",
    "\n",
    "            # 2. –î–ê–¢–ê (–†–∞–∑–¥–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã)\n",
    "            date_raw = \"Unknown\"\n",
    "            day_node = item.find('div', class_='list-day')\n",
    "            month_node = item.find('div', class_='list-month')\n",
    "            time_node = item.find('div', class_='list-time')\n",
    "\n",
    "            if day_node and month_node:\n",
    "                d_txt = day_node.get_text(strip=True)\n",
    "                m_txt = month_node.get_text(strip=True)\n",
    "                t_txt = time_node.get_text(strip=True) if time_node else \"\"\n",
    "                date_raw = f\"{d_txt} {m_txt} {t_txt}\".strip()\n",
    "\n",
    "            # 3. –°–°–´–õ–ö–ê –ò –°–¢–ê–¢–£–°\n",
    "            url = \"\"\n",
    "            status = \"available\"\n",
    "\n",
    "            other_block = item.find('div', class_='list-other')\n",
    "            if other_block:\n",
    "                btn = other_block.find('a', href=True)\n",
    "                if btn:\n",
    "                    link = btn['href']\n",
    "                    url = link if link.startswith('http') else f\"{BASE_URL_SOBOR}{link}\"\n",
    "                    btn_text = btn.get_text(strip=True).upper()\n",
    "                    if \"SOLD\" in btn_text or \"–ü–†–û–î–ê–ù–û\" in btn_text:\n",
    "                        status = \"sold_out\"\n",
    "\n",
    "            if not url:\n",
    "                status = \"unknown\"\n",
    "\n",
    "            # 4. –ö–ê–†–¢–ò–ù–ö–ê\n",
    "            photo_url = \"\"\n",
    "            img_block = item.find('div', class_='list-img')\n",
    "            if img_block:\n",
    "                img = img_block.find('img')\n",
    "                if img and img.get('src'):\n",
    "                    src = img['src']\n",
    "                    if 'svg' not in src:\n",
    "                        photo_url = src if src.startswith('http') else f\"{BASE_URL_SOBOR}{src}\"\n",
    "\n",
    "            # 5. –û–ü–ò–°–ê–ù–ò–ï (Markdown)\n",
    "            description = \"\"\n",
    "            descr_node = item.find('div', class_='list-descr')\n",
    "            if descr_node:\n",
    "                # get_text(separator=\"\\n\") –∑–∞–º–µ–Ω—è–µ—Ç <br> –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Ç–µ–≥–∏ –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏\n",
    "                description = descr_node.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "            # –î–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—è\n",
    "            key = f\"{date_raw}_{title}\"\n",
    "            if key in unique_keys:\n",
    "                continue\n",
    "            unique_keys.add(key)\n",
    "\n",
    "            events_list.append({\n",
    "                \"title\": title,\n",
    "                \"date_raw\": date_raw,\n",
    "                \"ticket_status\": status,\n",
    "                \"url\": url,\n",
    "                \"photos\": [photo_url] if photo_url else [],\n",
    "                \"description\": description,\n",
    "                \"pushkin_card\": False,\n",
    "                \"location\": \"–ö–∞—Ñ–µ–¥—Ä–∞–ª—å–Ω—ã–π —Å–æ–±–æ—Ä\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events_list)}\")\n",
    "    return events_list\n",
    "\n",
    "async def run_sobor(browser):\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    page = await context.new_page()\n",
    "\n",
    "    data = await scrape_sobor_strict(page)\n",
    "    await context.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# –ß–ê–°–¢–¨ 3: –¢–†–ï–¢–¨–Ø–ö–û–í–°–ö–ê–Ø –ì–ê–õ–ï–†–ï–Ø\n",
    "# ==========================================\n",
    "\n",
    "BASE_URL_TRETYAKOV = \"https://kaliningrad.tretyakovgallery.ru\"\n",
    "\n",
    "def normalize_tretyakov_date(date_raw):\n",
    "    \"\"\"\n",
    "    –ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–æ–∫ –≤–∏–¥–∞:\n",
    "    - \"–£–∂–µ –∏–¥–µ—Ç 4 —è–Ω–≤–∞—Ä—è –≤ 17:00\" -> (\"2025-01-04\", \"17:00\")\n",
    "    - \"–°–∫–æ—Ä–æ –° 25 –¥–µ–∫–∞–±—Ä—è –ø–æ 10 —è–Ω–≤–∞—Ä—è\" -> (\"2024-12-25\", \"00:00\") (–±–µ—Ä–µ–º –¥–∞—Ç—É –Ω–∞—á–∞–ª–∞)\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    \n",
    "    if not date_raw:\n",
    "        return None, None\n",
    "\n",
    "    text = date_raw.strip().lower()\n",
    "    \n",
    "    # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º—è\n",
    "    time_match = re.search(r'(\\d{1,2}):(\\d{2})', text)\n",
    "    parsed_time = \"00:00\"\n",
    "    if time_match:\n",
    "        parsed_time = f\"{int(time_match.group(1)):02d}:{int(time_match.group(2)):02d}\"\n",
    "\n",
    "    # 2. –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–∞\n",
    "    text = re.sub(r\"—É–∂–µ –∏–¥–µ—Ç|—Å–∫–æ—Ä–æ\", \"\", text).strip()\n",
    "    \n",
    "    # 3. –ü–æ–∏—Å–∫ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –∏–ª–∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è\n",
    "    range_match = re.search(r\"—Å\\s+(\\d{1,2})\\s+([–∞-—è]+)\", text)\n",
    "    \n",
    "    day = None\n",
    "    month_name = None\n",
    "    \n",
    "    if range_match:\n",
    "        day = int(range_match.group(1))\n",
    "        month_name = range_match.group(2)\n",
    "    else:\n",
    "        # \"2 –∏ 8 —è–Ω–≤–∞—Ä—è\" - –∏—â–µ–º –ª—é–±–æ–π –º–µ—Å—è—Ü\n",
    "        ru_months = [\"—è–Ω–≤–∞—Ä—è\", \"—Ñ–µ–≤—Ä–∞–ª—è\", \"–º–∞—Ä—Ç–∞\", \"–∞–ø—Ä–µ–ª—è\", \"–º–∞—è\", \"–∏—é–Ω—è\", \n",
    "                     \"–∏—é–ª—è\", \"–∞–≤–≥—É—Å—Ç–∞\", \"—Å–µ–Ω—Ç—è–±—Ä—è\", \"–æ–∫—Ç—è–±—Ä—è\", \"–Ω–æ—è–±—Ä—è\", \"–¥–µ–∫–∞–±—Ä—è\"]\n",
    "        \n",
    "        found_month = None\n",
    "        for m in ru_months:\n",
    "            if m in text:\n",
    "                found_month = m\n",
    "                break\n",
    "        \n",
    "        if found_month:\n",
    "            month_name = found_month\n",
    "            # –ò—â–µ–º –ø–µ—Ä–≤–æ–µ —á–∏—Å–ª–æ\n",
    "            digits = re.findall(r\"\\d+\", text)\n",
    "            if digits:\n",
    "                day = int(digits[0])\n",
    "\n",
    "    if not day or not month_name:\n",
    "        simple = re.search(r\"(\\d{1,2})\\s+([–∞-—è]+)\", text)\n",
    "        if simple:\n",
    "            day = int(simple.group(1))\n",
    "            month_name = simple.group(2)\n",
    "\n",
    "    # 4. –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Å—è—Ü–∞\n",
    "    MONTHS = {\n",
    "        \"—è–Ω–≤–∞—Ä—è\": 1, \"—Ñ–µ–≤—Ä–∞–ª—è\": 2, \"–º–∞—Ä—Ç–∞\", \"–∞–ø—Ä–µ–ª—è\": 4, \"–º–∞—è\": 5, \"–∏—é–Ω—è\": 6,\n",
    "        \"–∏—é–ª—è\": 7, \"–∞–≤–≥—É—Å—Ç–∞\": 8, \"—Å–µ–Ω—Ç—è–±—Ä—è\": 9, \"–æ–∫—Ç—è–±—Ä—è\": 10, \"–Ω–æ—è–±—Ä—è\": 11, \"–¥–µ–∫–∞–±—Ä—è\": 12\n",
    "    }\n",
    "    month = MONTHS.get(month_name)\n",
    "            \n",
    "    if not month:\n",
    "        return None, parsed_time\n",
    "\n",
    "    # 5. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ–¥–∞\n",
    "    today = datetime.date.today()\n",
    "    year = today.year\n",
    "    \n",
    "    try:\n",
    "        if today.month == 12 and month == 1:\n",
    "             year += 1\n",
    "        elif month < today.month and (today.month - month) > 4:\n",
    "             year += 1\n",
    "             \n",
    "        event_date = datetime.date(year, month, day)\n",
    "        return event_date.isoformat(), parsed_time\n",
    "    except ValueError:\n",
    "        return None, parsed_time\n",
    "\n",
    "\n",
    "async def scrape_tretyakov_schedule(page):\n",
    "    url = f\"{BASE_URL_TRETYAKOV}/events/\"\n",
    "    print(f\"\\nüñºÔ∏è [–¢—Ä–µ—Ç—å—è–∫–æ–≤–∫–∞] –≠—Ç–∞–ø 1: –°–∫–∞–Ω–∏—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è –Ω–∞ {url}...\")\n",
    "\n",
    "    try:\n",
    "        await page.goto(url, timeout=90000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(3000)\n",
    "\n",
    "        for _ in range(3):\n",
    "            await page.mouse.wheel(0, 4000)\n",
    "            await page.wait_for_timeout(random.randint(1000, 2000))\n",
    "\n",
    "        content = await page.content()\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        events_list = []\n",
    "        seen_ids = set()\n",
    "\n",
    "        cards = soup.select('.card')\n",
    "        for card in cards:\n",
    "            title_tag = card.select_one('.card_info .card_info-top .card_title')\n",
    "            if not title_tag:\n",
    "                continue\n",
    "            title = title_tag.get_text(strip=True)\n",
    "\n",
    "            link_tag = card.select_one('.card_info-top a[href]')\n",
    "            href = link_tag.get('href') if link_tag else \"\"\n",
    "            if href and href.startswith(\"//\"):\n",
    "                href = f\"https:{href}\"\n",
    "            elif href and href.startswith('/'):\n",
    "                href = f\"{BASE_URL_TRETYAKOV}{href}\"\n",
    "            \n",
    "            # --- –§–û–¢–û ---\n",
    "            img_url = \"\"\n",
    "            img_tag = card.select_one('img.card_img')\n",
    "            if img_tag and img_tag.get('src'):\n",
    "                img_url = img_tag['src']\n",
    "            if not img_url:\n",
    "                img_div = card.select_one('div.card_img')\n",
    "                style = img_div.get('style', '') if img_div else \"\"\n",
    "                match = re.search(r\"url\\((.*?)\\)\", style)\n",
    "                if match:\n",
    "                    img_url = match.group(1).strip().strip('\"').strip(\"'\")\n",
    "            if img_url and img_url.startswith('/'):\n",
    "                img_url = f\"{BASE_URL_TRETYAKOV}{img_url}\"\n",
    "\n",
    "            # --- –î–ê–¢–ê ---\n",
    "            date_text = \"\"\n",
    "            info = card.select_one('.card_info')\n",
    "            if info:\n",
    "                bottom = info.select_one('.card_info-bottom') or info\n",
    "                date_text = bottom.get_text(\" \", strip=True)\n",
    "\n",
    "            if title:\n",
    "                date_text = date_text.replace(title, \"\").strip()\n",
    "\n",
    "            clean_date = re.sub(r\"–ö—É–ø–∏—Ç—å\\s*–±–∏–ª–µ—Ç|–ö—É–ø–∏—Ç—å|–ë–∏–ª–µ—Ç—ã|–ü–æ–¥—Ä–æ–±–Ω–µ–µ\", \"\", date_text, flags=re.IGNORECASE).strip()\n",
    "            clean_date = re.sub(r\"\\b(–ê—Ç—Ä–∏—É–º|–ö–∏–Ω–æ–∑–∞–ª)\\b\", \"\", clean_date, flags=re.IGNORECASE).strip()\n",
    "            \n",
    "            parsed_date, parsed_time = normalize_tretyakov_date(clean_date)\n",
    "\n",
    "            card_text = card.get_text(\" \", strip=True).upper()\n",
    "            if \"–ê–¢–†–ò–£–ú\" in card_text:\n",
    "                location = \"–ê—Ç—Ä–∏—É–º\"\n",
    "            elif \"–ö–ò–ù–û–ó–ê–õ\" in card_text:\n",
    "                location = \"–ö–∏–Ω–æ–∑–∞–ª\"\n",
    "            else:\n",
    "                location = \"–§–∏–ª–∏–∞–ª –¢—Ä–µ—Ç—å—è–∫–æ–≤—Å–∫–æ–π –≥–∞–ª–µ—Ä–µ–∏, –ü–∞—Ä–∞–¥–Ω–∞—è –Ω–∞–±. 3, #–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥\"\n",
    "\n",
    "            event_id = f\"{clean_date}_{title}_{href}\"\n",
    "            if event_id in seen_ids:\n",
    "                continue\n",
    "            seen_ids.add(event_id)\n",
    "\n",
    "            events_list.append({\n",
    "                \"title\": title,\n",
    "                \"date_raw\": clean_date,\n",
    "                \"parsed_date\": parsed_date,\n",
    "                \"parsed_time\": parsed_time,\n",
    "                \"ticket_status\": \"unknown\",\n",
    "                \"url\": href,\n",
    "                \"photos\": [img_url] if img_url else [],\n",
    "                \"location\": location,\n",
    "            })\n",
    "\n",
    "        print(f\"‚úÖ [–¢—Ä–µ—Ç—å—è–∫–æ–≤–∫–∞] –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events_list)}\")\n",
    "        return events_list\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå [–¢—Ä–µ—Ç—å—è–∫–æ–≤–∫–∞] –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è: {e}\")\n",
    "        return []\n",
    "\n",
    "async def _tretyakov_interact_and_get_price(page, target_date=None, target_time=None):\n",
    "    \"\"\"\n",
    "    –°–¢–†–ê–¢–ï–ì–ò–Ø:\n",
    "    1. Check Price (–µ—Å–ª–∏ –µ—Å—Ç—å - –≤—ã—Ö–æ–¥).\n",
    "    2. Try STANDARD Selectors (exact classes) -> –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∏–≤–µ–Ω—Ç–æ–≤.\n",
    "    3. Try FUZZY Selectors (JS lookup) -> –¥–ª—è 'Fairy Tale' –∏ —Å–ª–æ–∂–Ω—ã—Ö.\n",
    "    4. Click Time -> Parse Price.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Wait for widget container or price\n",
    "        # –®–∏—Ä–æ–∫–∏–π –≤–µ–π—Ç, —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å —Å—Ä–∞–∑—É\n",
    "        try:\n",
    "            await page.wait_for_selector('div[class*=\"calendar\"], .skin-inner, app-root, .wrapper, .page-buy-container', timeout=8000)\n",
    "        except:\n",
    "             pass\n",
    "\n",
    "        # === CHECK 0: Pre-existing price ===\n",
    "        text_init = await page.inner_text(\"body\")\n",
    "        if re.search(r\"\\d+\\s*(?:‚ÇΩ|—Ä—É–±)\", text_init):\n",
    "            # Price visible?\n",
    "            pass \n",
    "\n",
    "        await page.wait_for_timeout(1000)\n",
    "\n",
    "        click_success = False\n",
    "        target_day = None\n",
    "        if target_date:\n",
    "            try:\n",
    "                target_day = int(target_date.split('-')[2])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if target_day:\n",
    "            print(f\"   ...Target Day: {target_day}\")\n",
    "            \n",
    "            # --- ATTEMPT 1: STANDARD SELECTORS (Strict) ---\n",
    "            # –≠—Ç–æ —Ä–∞–±–æ—Ç–∞–ª–æ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–æ–±—ã—Ç–∏–π —Ä–∞–Ω–µ–µ\n",
    "            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–æ–º day/cell –∏ —Ç–æ—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º\n",
    "            try:\n",
    "                # Selectors used in typical calendars\n",
    "                candidates = await page.locator('.cell:not(.day-header), .day, .date-item, span[class*=\"day\"]').all()\n",
    "                for cand in candidates:\n",
    "                    if not await cand.is_visible(): continue\n",
    "                    \n",
    "                    # Check text Exact Match\n",
    "                    txt = (await cand.inner_text()).strip()\n",
    "                    if txt == str(target_day):\n",
    "                        # Found it!\n",
    "                        cls = await cand.get_attribute('class') or \"\"\n",
    "                        if \"disabled\" in cls or \"sold\" in cls: continue\n",
    "                        \n",
    "                        await cand.click(force=True)\n",
    "                        click_success = True\n",
    "                        print(f\"   >>> [Standard] Clicked date {target_day}\")\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"   Standard click error: {e}\")\n",
    "\n",
    "            # --- ATTEMPT 2: FUZZY SELECTORS (JS) ---\n",
    "            # –ï—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª (–∫–∞–∫ –≤ Fairy Tale)\n",
    "            if not click_success:\n",
    "                print(f\"   ...Attempts Fuzzy Search for {target_day}\")\n",
    "                try:\n",
    "                    candidates = await page.evaluate_handle(r\"\"\"\n",
    "                        (day) => {\n",
    "                            const els = Array.from(document.querySelectorAll('div, span, button, td, a'));\n",
    "                            const matches = els.filter(el => {\n",
    "                                if (el.offsetParent === null) return false;\n",
    "                                const cls = (el.className || \"\").toString();\n",
    "                                if (cls.includes('disabled') || cls.includes('sold')) return false;\n",
    "                                \n",
    "                                const txt = el.innerText.trim();\n",
    "                                // Regex: word boundary check for day number\n",
    "                                // e.g. matches \"29\" in \"–ü–ù 29\" but NOT \"290\" or \"129\"\n",
    "                                const dayRx = new RegExp(\"(^|\\\\D)\" + day + \"(\\\\D|$)\", \"i\");\n",
    "                                if (!dayRx.test(txt)) return false;\n",
    "                                \n",
    "                                // Avoid long text blocks (descriptions)\n",
    "                                if (txt.length > 50) return false;\n",
    "                                \n",
    "                                // Avoid obvious non-date things if possible?\n",
    "                                return true;\n",
    "                            });\n",
    "                            // Prefer shorter matches (exact button vs container)\n",
    "                            matches.sort((a,b) => a.innerText.length - b.innerText.length);\n",
    "                            return matches.length > 0 ? matches[0] : null;\n",
    "                        }\n",
    "                    \"\"\", target_day)\n",
    "                    \n",
    "                    if candidates.as_element():\n",
    "                        await candidates.as_element().click()\n",
    "                        click_success = True\n",
    "                        print(f\"   >>> [Fuzzy] Clicked date {target_day}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Fuzzy click error: {e}\")\n",
    "\n",
    "        # Fallback date (Any date 1-31) if target specific failed\n",
    "        if not click_success:\n",
    "             print(\"   ...Fallback: Clicking first available date 1-31\")\n",
    "             try:\n",
    "                 candidates = await page.evaluate_handle(r\"\"\"\n",
    "                   () => {\n",
    "                        const els = Array.from(document.querySelectorAll('div, span, button, td, a'));\n",
    "                        const matches = els.filter(el => {\n",
    "                            if (el.offsetParent === null) return false;\n",
    "                            const cls = (el.className || \"\").toString();\n",
    "                            if (cls.includes('disabled') || cls.includes('sold')) return false;\n",
    "                            \n",
    "                            const txt = el.innerText.trim();\n",
    "                            // STRICT: Matches exactly a number 1-31 alone, or matches \"Mon 29\" format\n",
    "                            // Checking for explicit 1-31 range match\n",
    "                            const dateMatch = txt.match(/(^|\\D)([1-9]|[12]\\d|3[01])(\\D|$)/);\n",
    "                            if (!dateMatch) return false;\n",
    "                            \n",
    "                            if (txt.length > 40) return false; \n",
    "                            return true;\n",
    "                        });\n",
    "                        matches.sort((a,b) => a.innerText.length - b.innerText.length);\n",
    "                        return matches.length > 0 ? matches[0] : null;\n",
    "                   }\n",
    "                 \"\"\")\n",
    "                 if candidates.as_element():\n",
    "                     txt_click = await candidates.as_element().inner_text()\n",
    "                     await candidates.as_element().click()\n",
    "                     print(f\"   >>> [Fallback] Clicked date: {txt_click[:10]}...\")\n",
    "             except Exception as e:\n",
    "                 print(f\"   Fallback error: {e}\")\n",
    "\n",
    "        # 3. TIME SELECTION\n",
    "        await page.wait_for_timeout(1500) \n",
    "        \n",
    "        target_hm = target_time if target_time else None\n",
    "        print(\"   ...Searching Time\")\n",
    "        \n",
    "        # Standard + Fuzzy Time Search\n",
    "        try:\n",
    "             time_el = await page.evaluate_handle(r\"\"\"\n",
    "                (tgt) => {\n",
    "                    const els = Array.from(document.querySelectorAll('div, span, button, td, a'));\n",
    "                    const matches = els.filter(el => {\n",
    "                        if (el.offsetParent === null) return false;\n",
    "                        const cls = (el.className || \"\").toString();\n",
    "                        if (cls.includes('disabled')) return false;\n",
    "                        \n",
    "                        const txt = el.innerText.trim();\n",
    "                        // STRICT HH:MM pattern\n",
    "                        const timeMatch = txt.match(/\\d{1,2}:\\d{2}/);\n",
    "                        if (!timeMatch) return false;\n",
    "                        \n",
    "                        // If target provided, must contain it\n",
    "                        if (tgt && !txt.includes(tgt)) return false;\n",
    "                        \n",
    "                        if (txt.length > 30) return false;\n",
    "                        return true;\n",
    "                    });\n",
    "                     matches.sort((a,b) => a.innerText.length - b.innerText.length);\n",
    "                     return matches.length > 0 ? matches[0] : null;\n",
    "                }\n",
    "             \"\"\", target_hm)\n",
    "             \n",
    "             if time_el.as_element():\n",
    "                 t_txt = await time_el.as_element().inner_text()\n",
    "                 await time_el.as_element().click()\n",
    "                 print(f\"   >>> Clicked time: {t_txt}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # 4. PRICE EXTRACTION\n",
    "        await page.wait_for_timeout(2000)\n",
    "\n",
    "        text = await page.inner_text(\"body\")\n",
    "        lower = text.lower()\n",
    "        \n",
    "        if \"–Ω–µ—Ç –±–∏–ª–µ—Ç–æ–≤\" in lower or \"sold out\" in lower or \"—Ä–∞—Å–ø—Ä–æ–¥–∞–Ω–æ\" in lower:\n",
    "            return \"sold_out\", None, None\n",
    "            \n",
    "        prices = []\n",
    "        matches = re.findall(r\"(\\d+)\\s*(?:‚ÇΩ|—Ä—É–±|—Ä\\.)\", text, flags=re.IGNORECASE)\n",
    "        for m in matches:\n",
    "            prices.append(int(m))\n",
    "            \n",
    "        if prices:\n",
    "            prices = sorted(set(prices))\n",
    "            return \"available\", prices[0], prices[-1]\n",
    "            \n",
    "        return \"unknown\", None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Interaction error: {e}\")\n",
    "        return \"unknown\", None, None\n",
    "\n",
    "\n",
    "async def scrape_tretyakov_ticket_price(context, url, parsed_date=None, parsed_time=None):\n",
    "    print(f\"üí≥ [–¢—Ä–µ—Ç—å—è–∫–æ–≤–∫–∞] –ü—Ä–æ–≤–µ—Ä—è–µ–º: {url}\")\n",
    "    if not url: return {\"ticket_status\": \"unknown\", \"ticket_price_min\": None, \"ticket_price_max\": None}\n",
    "\n",
    "    page = await context.new_page()\n",
    "    status, p_min, p_max = \"unknown\", None, None\n",
    "\n",
    "    try:\n",
    "        await page.goto(url, timeout=60000, wait_until='domcontentloaded')\n",
    "        status, p_min, p_max = await _tretyakov_interact_and_get_price(page, parsed_date, parsed_time)\n",
    "        print(f\"   Result: {status} Price: {p_min}-{p_max}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    finally:\n",
    "        await page.close()\n",
    "    \n",
    "    return {\n",
    "        \"ticket_status\": status,\n",
    "        \"ticket_price_min\": p_min,\n",
    "        \"ticket_price_max\": p_max,\n",
    "    }\n",
    "\n",
    "async def run_tretyakov(browser):\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    await context.route(\"**/*{google,yandex,metrika,analytics}*\", lambda route: route.abort())\n",
    "    page = await context.new_page()\n",
    "\n",
    "    schedule = await scrape_tretyakov_schedule(page)\n",
    "    if not schedule:\n",
    "        await context.close()\n",
    "        return []\n",
    "\n",
    "    for item in schedule:\n",
    "        url = item.get(\"url\", \"\")\n",
    "        if not url or \"tickets\" not in url: continue\n",
    "        \n",
    "        res = await scrape_tretyakov_ticket_price(context, url, item.get(\"parsed_date\"), item.get(\"parsed_time\"))\n",
    "        item.update(res)\n",
    "        await page.wait_for_timeout(random.randint(500, 1000))\n",
    "\n",
    "    await context.close()\n",
    "    return schedule\n",
    "\n",
    "\n",
    "# --- –ó–ê–ü–£–°–ö ---\n",
    "async def main():\n",
    "    config = {}\n",
    "    target = \"all\"\n",
    "    if os.path.exists('run_config.json'):\n",
    "        config = json.load(open('run_config.json'))\n",
    "        target = config.get('target_source', 'all')\n",
    "\n",
    "    print(\"üöÄ –ó–ê–ü–£–°–ö –°–ë–û–†–ê –î–ê–ù–ù–´–•...\")\n",
    "    print(\"‚ÑπÔ∏è [INFO] –ó–∞–ø—É—Å–∫–∞–µ–º –î—Ä–∞–º—Ç–µ–∞—Ç—Ä, –ú—É–∑—Ç–µ–∞—Ç—Ä, –ö–∞—Ñ–µ–¥—Ä–∞–ª—å–Ω—ã–π —Å–æ–±–æ—Ä –∏ –¢—Ä–µ—Ç—å—è–∫–æ–≤—Å–∫—É—é –≥–∞–ª–µ—Ä–µ—é.\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "\n",
    "        data_dram = []\n",
    "        data_muz = []\n",
    "        data_sobor = []\n",
    "        data_tretyakov = []\n",
    "\n",
    "        if target in (\"all\", \"dramteatr\"):\n",
    "            data_dram = await run_dramteatr(browser)\n",
    "            results[\"dramteatr\"] = data_dram\n",
    "            if data_dram:\n",
    "                with open('dramteatr.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(data_dram, f, ensure_ascii=False, indent=4)\n",
    "                pd.DataFrame(data_dram).to_csv('dramteatr.csv', index=False)\n",
    "                print(f\"üíæ Dramteatr saved: {len(data_dram)}\")\n",
    "\n",
    "        if target in (\"all\", \"muzteatr\"):\n",
    "            data_muz = await run_muzteatr(browser)\n",
    "            results[\"muzteatr\"] = data_muz\n",
    "            if data_muz:\n",
    "                with open('muzteatr.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(data_muz, f, ensure_ascii=False, indent=4)\n",
    "                df_muz = pd.DataFrame(data_muz)\n",
    "                if 'photos' in df_muz.columns:\n",
    "                    df_muz['photos'] = df_muz['photos'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else x)\n",
    "                df_muz.to_csv('muzteatr.csv', index=False)\n",
    "                print(f\"üíæ Muzteatr saved: {len(data_muz)}\")\n",
    "\n",
    "        if target in (\"all\", \"sobor\"):\n",
    "            data_sobor = await run_sobor(browser)\n",
    "            results[\"sobor\"] = data_sobor\n",
    "\n",
    "        if target in (\"all\", \"tretyakov\"):\n",
    "            data_tretyakov = await run_tretyakov(browser)\n",
    "            results[\"tretyakov\"] = data_tretyakov\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    all_data = []\n",
    "    for chunk in results.values():\n",
    "        if chunk:\n",
    "            all_data.extend(chunk)\n",
    "\n",
    "    if all_data:\n",
    "        results[\"all\"] = all_data\n",
    "\n",
    "    return results\n",
    "\n",
    "result = await main()\n",
    "\n",
    "sobor_data = result.get(\"sobor\", []) if result else []\n",
    "tretyakov_data = result.get(\"tretyakov\", []) if result else []\n",
    "all_data = result.get(\"all\", []) if result else []\n",
    "\n",
    "if sobor_data:\n",
    "    with open('sobor.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(sobor_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    df = pd.DataFrame(sobor_data)\n",
    "    if 'photos' in df.columns:\n",
    "        df['photo_preview'] = df['photos'].apply(lambda x: x[0] if x else \"\")\n",
    "\n",
    "    # –û–±—Ä–µ–∑–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ø—Ä–µ–≤—å—é –≤ –∫–æ–Ω—Å–æ–ª–∏\n",
    "    if 'description' in df.columns:\n",
    "        df['desc_preview'] = df['description'].apply(lambda x: (x[:50] + '...') if len(x) > 50 else x)\n",
    "\n",
    "    df.drop(columns=['photos', 'desc_preview'], inplace=True, errors='ignore')\n",
    "    df.to_csv('sobor.csv', index=False)\n",
    "\n",
    "if tretyakov_data:\n",
    "    with open('tretyakov.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(tretyakov_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    df_tretyakov = pd.DataFrame(tretyakov_data)\n",
    "    if 'photos' in df_tretyakov.columns:\n",
    "        df_tretyakov['photos'] = df_tretyakov['photos'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else x)\n",
    "    df_tretyakov.to_csv('tretyakov.csv', index=False)\n",
    "\n",
    "if all_data:\n",
    "    df_all = pd.DataFrame(all_data)\n",
    "    if 'photos' in df_all.columns:\n",
    "        df_all['photo_preview'] = df_all['photos'].apply(lambda x: x[0] if isinstance(x, list) and x else \"\")\n",
    "    if 'description' in df_all.columns:\n",
    "        df_all['desc_preview'] = df_all['description'].apply(lambda x: (x[:50] + '...') if len(x) > 50 else x)\n",
    "    else:\n",
    "        df_all['desc_preview'] = \"\"\n",
    "\n",
    "    print(f\"\\nüéâ –ò–¢–û–ì: {len(all_data)} —Å–æ–±—ã—Ç–∏–π.\")\n",
    "    pd.set_option('display.max_colwidth', 40)\n",
    "    print(df_all[['date_raw', 'title', 'desc_preview']].head(10))\n",
    "else:\n",
    "    print(\"–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\")\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}