{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "# --- 1. –£–°–¢–ê–ù–û–í–ö–ê –ë–ò–ë–õ–ò–û–¢–ï–ö ---\n",
    "def install_libs():\n",
    "    try:\n",
    "        import playwright\n",
    "        import bs4\n",
    "    except ImportError:\n",
    "        print(\"‚è≥ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"playwright\", \"beautifulsoup4\", \"pandas\"])\n",
    "        os.system(\"playwright install chromium\")\n",
    "        os.system(\"playwright install-deps\")\n",
    "        print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≥–æ—Ç–æ–≤—ã.\")\n",
    "\n",
    "install_libs()\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ==========================================\n",
    "# –ß–ê–°–¢–¨ 1: –î–†–ê–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –¢–ï–ê–¢–† (DRAMTEATR)\n",
    "# ==========================================\n",
    "\n",
    "async def scrape_dram_schedule(page):\n",
    "    url = \"https://dramteatr39.ru/afisha\"\n",
    "    print(f\"\\nüé≠ [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –≠—Ç–∞–ø 1: –°–∫–∞–Ω–∏—Ä—É–µ–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ {url}...\")\n",
    "\n",
    "    try:\n",
    "        await page.goto(url, timeout=90000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(3000)\n",
    "\n",
    "        # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞\n",
    "        scrolls = 6\n",
    "        for _ in range(scrolls):\n",
    "            await page.mouse.wheel(0, 4000)\n",
    "            await page.wait_for_timeout(random.randint(1000, 3000))\n",
    "\n",
    "        content = await page.content()\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        events_list = []\n",
    "        event_links = soup.find_all('a', href=re.compile(r'/spektakli/'))\n",
    "        seen_identifiers = set()\n",
    "\n",
    "        for link in event_links:\n",
    "            href = link.get('href')\n",
    "            full_link = f\"https://dramteatr39.ru{href}\" if href.startswith('/') else href\n",
    "\n",
    "            title = link.get_text(strip=True)\n",
    "            if len(title) < 2:\n",
    "                continue\n",
    "\n",
    "            date_block = link.find_parent(class_='affiche__date-block')\n",
    "            card = None\n",
    "            if date_block:\n",
    "                card = date_block.find_parent() or date_block\n",
    "            else:\n",
    "                card = link.find_parent()\n",
    "                if card:\n",
    "                    date_block = card.select_one('.affiche__date-block')\n",
    "\n",
    "            if not card:\n",
    "                continue\n",
    "\n",
    "            card_text = card.get_text(\" \", strip=True).upper()\n",
    "\n",
    "            # –î–∞—Ç–∞\n",
    "            date_text = \"–î–∞—Ç–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\"\n",
    "            day_text = \"\"\n",
    "            month_text = \"\"\n",
    "            time_text = \"\"\n",
    "\n",
    "            if date_block:\n",
    "                day_node = date_block.select_one('.date-list__day')\n",
    "                month_node = date_block.select_one('.date-list__month')\n",
    "                time_node = date_block.select_one('.date-list__time')\n",
    "\n",
    "                if day_node:\n",
    "                    day_text = day_node.get_text(strip=True)\n",
    "                if month_node:\n",
    "                    month_text = month_node.get_text(strip=True)\n",
    "                if time_node:\n",
    "                    spans = [s.get_text(strip=True) for s in time_node.find_all('span') if s.get_text(strip=True)]\n",
    "                    if spans:\n",
    "                        time_text = spans[-1]\n",
    "                    else:\n",
    "                        raw_time = time_node.get_text(\" \", strip=True)\n",
    "                        parts = [p for p in raw_time.split() if \":\" in p]\n",
    "                        if parts:\n",
    "                            time_text = parts[-1]\n",
    "\n",
    "            if day_text and month_text:\n",
    "                date_text = f\"{day_text} {month_text}\"\n",
    "                if time_text:\n",
    "                    date_text = f\"{date_text} {time_text}\"\n",
    "\n",
    "            # –°—Ç–∞—Ç—É—Å\n",
    "            status = \"available\"\n",
    "            if \"–ë–ò–õ–ï–¢–û–í –ù–ï–¢\" in card_text or \"–ü–†–û–î–ê–ù–û\" in card_text:\n",
    "                status = \"sold_out\"\n",
    "\n",
    "            # –°—Ü–µ–Ω–∞\n",
    "            if \"–û–°–ù–û–í–ù–ê–Ø\" in card_text:\n",
    "                scene = \"–û—Å–Ω–æ–≤–Ω–∞—è\"\n",
    "            elif \"–°–†–ï–î–ù–Ø–Ø\" in card_text:\n",
    "                scene = \"–°—Ä–µ–¥–Ω—è—è\"\n",
    "            else:\n",
    "                scene = \"–ú–∞–ª–∞—è\"\n",
    "\n",
    "            event_id = f\"{date_text}_{title}\"\n",
    "            if event_id in seen_identifiers:\n",
    "                continue\n",
    "            seen_identifiers.add(event_id)\n",
    "\n",
    "            events_list.append({\n",
    "                \"title\": title,\n",
    "                \"date_raw\": date_text,\n",
    "                \"ticket_status\": status,\n",
    "                \"scene\": scene,\n",
    "                \"url\": full_link,\n",
    "                \"location\": \"–î—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ–∞—Ç—Ä\"\n",
    "            })\n",
    "\n",
    "        print(f\"‚úÖ [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events_list)}\")\n",
    "        return events_list\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è: {e}\")\n",
    "        return []\n",
    "\n",
    "async def scrape_dram_details(context, unique_urls):\n",
    "    print(f\"üïµÔ∏è‚Äç‚ôÇÔ∏è [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –°–±–æ—Ä –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {len(unique_urls)} —Å–ø–µ–∫—Ç–∞–∫–ª–µ–π...\")\n",
    "    details_map = {}\n",
    "    page = await context.new_page()\n",
    "\n",
    "    for url in unique_urls:\n",
    "        try:\n",
    "            await page.goto(url, timeout=45000, wait_until='domcontentloaded')\n",
    "            try:\n",
    "                await page.wait_for_selector('h1', timeout=3000)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            content = await page.content()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            full_text = soup.get_text(\" \", strip=True).upper()\n",
    "\n",
    "            is_pushkin = \"–ü–£–®–ö–ò–ù–°–ö–ê–Ø –ö–ê–†–¢–ê\" in full_text\n",
    "\n",
    "            images = []\n",
    "            for a in soup.find_all('a', href=re.compile(r'\\.(jpg|jpeg|png)$', re.IGNORECASE)):\n",
    "                img_url = a['href']\n",
    "                if img_url.startswith('/'):\n",
    "                    img_url = f\"https://dramteatr39.ru{img_url}\"\n",
    "                if img_url not in images:\n",
    "                    images.append(img_url)\n",
    "\n",
    "            if len(images) < 2:\n",
    "                for img in soup.select('div.swiper-slide img, .gallery img, .content img'):\n",
    "                    src = img.get('src')\n",
    "                    if src and 'logo' not in src:\n",
    "                        if src.startswith('/'):\n",
    "                            src = f\"https://dramteatr39.ru{src}\"\n",
    "                        if src not in images:\n",
    "                            images.append(src)\n",
    "\n",
    "            desc_text = \"\"\n",
    "            desc_div = soup.find('div', class_=re.compile(r'description|text-block'))\n",
    "            if not desc_div:\n",
    "                header = soup.find(string=re.compile(\"–û —Å–ø–µ–∫—Ç–∞–∫–ª–µ\"))\n",
    "                if header and header.parent:\n",
    "                    parent = header.find_parent('div')\n",
    "                    if parent:\n",
    "                        desc_div = parent\n",
    "            if desc_div:\n",
    "                desc_text = desc_div.get_text(\"\\n\", strip=True)\n",
    "\n",
    "            creators = \"\"\n",
    "            c_div = soup.find('div', class_=re.compile(r'creators|team'))\n",
    "            if c_div:\n",
    "                creators = c_div.get_text(\" | \", strip=True)\n",
    "\n",
    "            details_map[url] = {\n",
    "                \"pushkin_card\": is_pushkin,\n",
    "                \"photos\": images,\n",
    "                \"description\": desc_text[:1500],\n",
    "                \"creators\": creators[:500]\n",
    "            }\n",
    "        except Exception:\n",
    "            details_map[url] = {}\n",
    "\n",
    "    await page.close()\n",
    "    return details_map\n",
    "\n",
    "async def run_dramteatr(browser):\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    page = await context.new_page()\n",
    "\n",
    "    schedule = await scrape_dram_schedule(page)\n",
    "    if not schedule:\n",
    "        await context.close()\n",
    "        return []\n",
    "\n",
    "    unique_links = list(set(x['url'] for x in schedule))\n",
    "    details = await scrape_dram_details(context, unique_links)\n",
    "\n",
    "    await context.close()\n",
    "\n",
    "    final_data = []\n",
    "    for item in schedule:\n",
    "        det = details.get(item['url'], {})\n",
    "        final_data.append({**item, **det})\n",
    "\n",
    "    return final_data\n",
    "\n",
    "# ==========================================\n",
    "# –ß–ê–°–¢–¨ 2: –ú–£–ó–´–ö–ê–õ–¨–ù–´–ô –¢–ï–ê–¢–† (MUZTEATR)\n",
    "# ==========================================\n",
    "\n",
    "BASE_URL_MUZ = \"https://muzteatr39.ru\"\n",
    "\n",
    "async def find_muz_afisha_link(page):\n",
    "    try:\n",
    "        await page.goto(BASE_URL_MUZ, timeout=30000, wait_until='domcontentloaded')\n",
    "        link = page.get_by_role(\"link\", name=re.compile(\"–ê—Ñ–∏—à–∞\", re.IGNORECASE)).first\n",
    "        href = await link.get_attribute(\"href\")\n",
    "        if href:\n",
    "            return f\"{BASE_URL_MUZ}{href}\" if href.startswith(\"/\") else href\n",
    "    except:\n",
    "        pass\n",
    "    return f\"{BASE_URL_MUZ}/afisha\"\n",
    "\n",
    "async def scrape_muz_schedule(page, afisha_url):\n",
    "    print(f\"\\nüéª [–ú—É–∑—Ç–µ–∞—Ç—Ä] –≠—Ç–∞–ø 1: –°–∫–∞–Ω–∏—Ä—É–µ–º –∞—Ñ–∏—à—É: {afisha_url}...\")\n",
    "    try:\n",
    "        await page.goto(afisha_url, timeout=60000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(2000)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # –°–∫—Ä–æ–ª–ª\n",
    "    try:\n",
    "        for _ in range(3):\n",
    "            await page.mouse.wheel(0, 4000)\n",
    "            await page.wait_for_timeout(1000)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    content = await page.content()\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    events_list = []\n",
    "    unique_ids = set()\n",
    "    MONTHS_MAP = {'–Ø–ù–í–ê–†': '–Ø–ù–í–ê–†–Ø', '–§–ï–í–†–ê–õ': '–§–ï–í–†–ê–õ–Ø', '–ú–ê–†–¢': '–ú–ê–†–¢–ê', '–ê–ü–†–ï–õ': '–ê–ü–†–ï–õ–Ø', '–ú–ê–ô': '–ú–ê–Ø', '–ò–Æ–ù': '–ò–Æ–ù–Ø', '–ò–Æ–õ': '–ò–Æ–õ–Ø', '–ê–í–ì–£–°–¢': '–ê–í–ì–£–°–¢–ê', '–°–ï–ù–¢–Ø–ë–†': '–°–ï–ù–¢–Ø–ë–†–Ø', '–û–ö–¢–Ø–ë–†': '–û–ö–¢–Ø–ë–†–Ø', '–ù–û–Ø–ë–†': '–ù–û–Ø–ë–†–Ø', '–î–ï–ö–ê–ë–†': '–î–ï–ö–ê–ë–†–Ø'}\n",
    "    last_seen_month = \"?\"\n",
    "\n",
    "    rows = soup.find_all('div', class_=re.compile(r'row\\s+afisha'))\n",
    "    for row in rows:\n",
    "        date_col = row.find('div', class_=re.compile(r'afisha_data'))\n",
    "        day_str, month_str, time_str = \"?\", last_seen_month, \"00:00\"\n",
    "\n",
    "        if date_col:\n",
    "            col_text = date_col.get_text(\" \", strip=True).upper()\n",
    "            day_div = date_col.find('div', class_='afisha_nom')\n",
    "            if day_div:\n",
    "                day_str = day_div.get_text(strip=True)\n",
    "            else:\n",
    "                day_match = re.search(r'^(\\d{1,2})', col_text)\n",
    "                if day_match:\n",
    "                    day_str = day_match.group(1)\n",
    "\n",
    "            for root, full_name in MONTHS_MAP.items():\n",
    "                if root in col_text:\n",
    "                    month_str = full_name\n",
    "                    last_seen_month = full_name\n",
    "                    break\n",
    "\n",
    "            time_match = re.search(r'(\\d{1,2}:\\d{2})', col_text)\n",
    "            if time_match:\n",
    "                time_str = time_match.group(1).zfill(5)\n",
    "\n",
    "        if not day_str.isdigit():\n",
    "            continue\n",
    "        date_final = f\"{day_str} {month_str} {time_str}\"\n",
    "\n",
    "        title_div = row.find('div', class_='afisha_spek_title')\n",
    "        if not title_div:\n",
    "            continue\n",
    "\n",
    "        title = \"\"\n",
    "        link_tag = title_div.find('a')\n",
    "        if link_tag:\n",
    "            title = link_tag.get_text(strip=True)\n",
    "            href = link_tag.get('href')\n",
    "        else:\n",
    "            badge = title_div.find('span', class_='badge')\n",
    "            if badge:\n",
    "                badge.decompose()\n",
    "            title = title_div.get_text(strip=True)\n",
    "            href = None\n",
    "\n",
    "        if not href:\n",
    "            continue\n",
    "        full_link = f\"{BASE_URL_MUZ}{href}\" if href.startswith('/') else href\n",
    "\n",
    "        badge = row.find('span', class_='badge')\n",
    "        age = badge.get_text(strip=True) if badge else \"\"\n",
    "\n",
    "        row_text_full = row.get_text(\" \", strip=True).upper()\n",
    "        status = \"available\"\n",
    "        if \"–ë–ò–õ–ï–¢–û–í –ù–ï–¢\" in row_text_full or \"–ü–†–û–î–ê–ù–û\" in row_text_full:\n",
    "            status = \"sold_out\"\n",
    "\n",
    "        eid = f\"{date_final}_{full_link}\"\n",
    "        if eid in unique_ids:\n",
    "            continue\n",
    "        unique_ids.add(eid)\n",
    "\n",
    "        events_list.append({\n",
    "            \"title\": title,\n",
    "            \"date_raw\": date_final,\n",
    "            \"age_restriction\": age,\n",
    "            \"ticket_status\": status,\n",
    "            \"url\": full_link,\n",
    "            \"location\": \"–ú—É–∑—ã–∫–∞–ª—å–Ω—ã–π —Ç–µ–∞—Ç—Ä\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ [–ú—É–∑—Ç–µ–∞—Ç—Ä] –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events_list)}\")\n",
    "    return events_list\n",
    "\n",
    "async def scrape_muz_details(context, unique_urls):\n",
    "    print(f\"üïµÔ∏è‚Äç‚ôÇÔ∏è [–ú—É–∑—Ç–µ–∞—Ç—Ä] –°–±–æ—Ä –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {len(unique_urls)} —Å–æ–±—ã—Ç–∏–π...\")\n",
    "    details = {}\n",
    "    page = await context.new_page()\n",
    "    BAD_IMAGES = ['ulogin', 'provider', 'icon', 'logo', 'social', 'vk.com', 'ok.ru', 'blank', 'pixel']\n",
    "\n",
    "    for url in unique_urls:\n",
    "        try:\n",
    "            if not url or 'http' not in url:\n",
    "                continue\n",
    "            try:\n",
    "                await page.goto(url, timeout=30000, wait_until='domcontentloaded')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            content = await page.content()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            desc_text = \"\"\n",
    "            content_div = soup.find('div', class_=re.compile(r'detail-text|news-detail|item-text'))\n",
    "            if content_div:\n",
    "                raw = content_div.get_text(\"\\n\", strip=True)\n",
    "                lines = [l for l in raw.split('\\n') if len(l) > 30 and '–≤–æ–π—Ç–∏' not in l.lower()]\n",
    "                desc_text = \"\\n\".join(lines)[:2000]\n",
    "\n",
    "            images = []\n",
    "            for d in soup.find_all('div', style=re.compile(r'url\\(')):\n",
    "                match = re.search(r'url\\([\\'\\\"]?(.*?)[\\'\\\"]?\\)', d['style'])\n",
    "                if match:\n",
    "                    u = match.group(1)\n",
    "                    if u.startswith('/'):\n",
    "                        u = f\"{BASE_URL_MUZ}{u}\"\n",
    "                    if not any(b in u.lower() for b in BAD_IMAGES):\n",
    "                        images.append(u)\n",
    "\n",
    "            container = soup.find('div', class_=re.compile(r'container|main|content')) or soup\n",
    "            for img in container.find_all('img'):\n",
    "                src = img.get('src')\n",
    "                if src and not src.endswith('.svg') and not any(b in src.lower() for b in BAD_IMAGES):\n",
    "                    if src.startswith('/'):\n",
    "                        src = f\"{BASE_URL_MUZ}{src}\"\n",
    "                    images.append(src)\n",
    "\n",
    "            details[url] = {\n",
    "                \"pushkin_card\": \"–ü–£–®–ö–ò–ù–°–ö–ê–Ø –ö–ê–†–¢–ê\" in soup.get_text().upper(),\n",
    "                \"description\": desc_text,\n",
    "                \"photos\": list(set(images))\n",
    "            }\n",
    "        except:\n",
    "            details[url] = {}\n",
    "\n",
    "    await page.close()\n",
    "    return details\n",
    "\n",
    "async def run_muzteatr(browser):\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    page = await context.new_page()\n",
    "\n",
    "    afisha_url = await find_muz_afisha_link(page)\n",
    "    events = await scrape_muz_schedule(page, afisha_url)\n",
    "\n",
    "    if not events:\n",
    "        await context.close()\n",
    "        return []\n",
    "\n",
    "    urls = list(set(x['url'] for x in events))\n",
    "    details = await scrape_muz_details(context, urls)\n",
    "\n",
    "    await context.close()\n",
    "\n",
    "    final = []\n",
    "    for ev in events:\n",
    "        det = details.get(ev['url'], {})\n",
    "        final.append({**ev, **det})\n",
    "    return final\n",
    "\n",
    "BASE_URL_SOBOR = \"https://sobor39.ru\"\n",
    "\n",
    "# --- 2. –ü–ê–†–°–ï–† –°–û–ë–û–†–ê ---\n",
    "async def scrape_sobor_strict(page):\n",
    "    target_url = \"https://sobor39.ru/events/concerts/night/\"\n",
    "    print(f\"‚õ™ –ó–∞—Ö–æ–¥–∏–º –Ω–∞ {target_url}...\")\n",
    "\n",
    "    try:\n",
    "        await page.goto(target_url, timeout=90000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(5000)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏: {e}. –ü—Ä–æ–±—É–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–µ–º, —á—Ç–æ –µ—Å—Ç—å.\")\n",
    "\n",
    "    # --- –ü–†–û–ö–†–£–¢–ö–ê ---\n",
    "    print(\"‚¨áÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∫—É –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ —Å–æ–±—ã—Ç–∏–π...\")\n",
    "\n",
    "    for i in range(10):\n",
    "        await page.mouse.wheel(0, 5000)\n",
    "        await page.wait_for_timeout(3000)\n",
    "\n",
    "        try:\n",
    "            more_btns = page.locator(\"text=/–ü–æ–∫–∞–∑–∞—Ç—å –µ—â[—ë–µ]|–ó–∞–≥—Ä—É–∑–∏—Ç—å/i\")\n",
    "            if await more_btns.count() > 0:\n",
    "                if await more_btns.first.is_visible():\n",
    "                    await more_btns.first.click()\n",
    "                    await page.wait_for_timeout(2000)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            print(f\"   ...–ø—Ä–æ–∫—Ä—É—Ç–∫–∞ {i+1}/10\")\n",
    "\n",
    "    content = await page.content()\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    events_list = []\n",
    "    unique_keys = set()\n",
    "\n",
    "    items = soup.find_all('div', class_='list-item')\n",
    "    print(f\"üîç –ù–∞–π–¥–µ–Ω–æ –±–ª–æ–∫–æ–≤ .list-item: {len(items)}\")\n",
    "\n",
    "    for item in items:\n",
    "        try:\n",
    "            # 1. –ó–ê–ì–û–õ–û–í–û–ö\n",
    "            title_tag = item.find('div', class_='list-t')\n",
    "            if not title_tag:\n",
    "                continue\n",
    "            title = title_tag.get_text(strip=True)\n",
    "\n",
    "            # 2. –î–ê–¢–ê (–†–∞–∑–¥–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã)\n",
    "            date_raw = \"Unknown\"\n",
    "            day_node = item.find('div', class_='list-day')\n",
    "            month_node = item.find('div', class_='list-month')\n",
    "            time_node = item.find('div', class_='list-time')\n",
    "\n",
    "            if day_node and month_node:\n",
    "                d_txt = day_node.get_text(strip=True)\n",
    "                m_txt = month_node.get_text(strip=True)\n",
    "                t_txt = time_node.get_text(strip=True) if time_node else \"\"\n",
    "                date_raw = f\"{d_txt} {m_txt} {t_txt}\".strip()\n",
    "\n",
    "            # 3. –°–°–´–õ–ö–ê –ò –°–¢–ê–¢–£–°\n",
    "            url = \"\"\n",
    "            status = \"available\"\n",
    "\n",
    "            other_block = item.find('div', class_='list-other')\n",
    "            if other_block:\n",
    "                btn = other_block.find('a', href=True)\n",
    "                if btn:\n",
    "                    link = btn['href']\n",
    "                    url = link if link.startswith('http') else f\"{BASE_URL_SOBOR}{link}\"\n",
    "                    btn_text = btn.get_text(strip=True).upper()\n",
    "                    if \"SOLD\" in btn_text or \"–ü–†–û–î–ê–ù–û\" in btn_text:\n",
    "                        status = \"sold_out\"\n",
    "\n",
    "            if not url:\n",
    "                status = \"unknown\"\n",
    "\n",
    "            # 4. –ö–ê–†–¢–ò–ù–ö–ê\n",
    "            photo_url = \"\"\n",
    "            img_block = item.find('div', class_='list-img')\n",
    "            if img_block:\n",
    "                img = img_block.find('img')\n",
    "                if img and img.get('src'):\n",
    "                    src = img['src']\n",
    "                    if 'svg' not in src:\n",
    "                        photo_url = src if src.startswith('http') else f\"{BASE_URL_SOBOR}{src}\"\n",
    "\n",
    "            # 5. –û–ü–ò–°–ê–ù–ò–ï (Markdown)\n",
    "            description = \"\"\n",
    "            descr_node = item.find('div', class_='list-descr')\n",
    "            if descr_node:\n",
    "                # get_text(separator=\"\\n\") –∑–∞–º–µ–Ω—è–µ—Ç <br> –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Ç–µ–≥–∏ –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏\n",
    "                description = descr_node.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "            # –î–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—è\n",
    "            key = f\"{date_raw}_{title}\"\n",
    "            if key in unique_keys:\n",
    "                continue\n",
    "            unique_keys.add(key)\n",
    "\n",
    "            events_list.append({\n",
    "                \"title\": title,\n",
    "                \"date_raw\": date_raw,\n",
    "                \"ticket_status\": status,\n",
    "                \"url\": url,\n",
    "                \"photos\": [photo_url] if photo_url else [],\n",
    "                \"description\": description,\n",
    "                \"pushkin_card\": False,\n",
    "                \"location\": \"–ö–∞—Ñ–µ–¥—Ä–∞–ª—å–Ω—ã–π —Å–æ–±–æ—Ä\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events_list)}\")\n",
    "    return events_list\n",
    "\n",
    "async def run_sobor(browser):\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    page = await context.new_page()\n",
    "\n",
    "    data = await scrape_sobor_strict(page)\n",
    "    await context.close()\n",
    "    return data\n",
    "\n",
    "# --- –ó–ê–ü–£–°–ö ---\n",
    "async def main():\n",
    "    print(\"üöÄ –ó–ê–ü–£–°–ö –°–ë–û–†–ê –î–ê–ù–ù–´–•...\")\n",
    "    print(\"‚ÑπÔ∏è [INFO] –ó–∞–ø—É—Å–∫–∞–µ–º –î—Ä–∞–º—Ç–µ–∞—Ç—Ä, –ú—É–∑—Ç–µ–∞—Ç—Ä –∏ –ö–∞—Ñ–µ–¥—Ä–∞–ª—å–Ω—ã–π —Å–æ–±–æ—Ä.\")\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "\n",
    "        data_dram = await run_dramteatr(browser)\n",
    "        if data_dram:\n",
    "            with open('dramteatr.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(data_dram, f, ensure_ascii=False, indent=4)\n",
    "            pd.DataFrame(data_dram).to_csv('dramteatr.csv', index=False)\n",
    "            print(f\"üíæ Dramteatr saved: {len(data_dram)}\")\n",
    "\n",
    "        data_muz = await run_muzteatr(browser)\n",
    "        if data_muz:\n",
    "            with open('muzteatr.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(data_muz, f, ensure_ascii=False, indent=4)\n",
    "            df_muz = pd.DataFrame(data_muz)\n",
    "            if 'photos' in df_muz.columns:\n",
    "                df_muz['photos'] = df_muz['photos'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else x)\n",
    "            df_muz.to_csv('muzteatr.csv', index=False)\n",
    "            print(f\"üíæ Muzteatr saved: {len(data_muz)}\")\n",
    "\n",
    "        data_sobor = await run_sobor(browser)\n",
    "        await browser.close()\n",
    "\n",
    "    return data_sobor\n",
    "\n",
    "result = await main()\n",
    "\n",
    "if result:\n",
    "    with open('sobor.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    if 'photos' in df.columns:\n",
    "        df['photo_preview'] = df['photos'].apply(lambda x: x[0] if x else \"\")\n",
    "\n",
    "    # –û–±—Ä–µ–∑–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ø—Ä–µ–≤—å—é –≤ –∫–æ–Ω—Å–æ–ª–∏\n",
    "    if 'description' in df.columns:\n",
    "        df['desc_preview'] = df['description'].apply(lambda x: (x[:50] + '...') if len(x) > 50 else x)\n",
    "\n",
    "    print(f\"\\nüéâ –ò–¢–û–ì: {len(result)} —Å–æ–±—ã—Ç–∏–π.\")\n",
    "    pd.set_option('display.max_colwidth', 40)\n",
    "    print(df[['date_raw', 'title', 'desc_preview']].head(10))\n",
    "\n",
    "    df.drop(columns=['photos', 'desc_preview'], inplace=True, errors='ignore')\n",
    "    df.to_csv('sobor.csv', index=False)\n",
    "else:\n",
    "    print(\"–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\")\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}