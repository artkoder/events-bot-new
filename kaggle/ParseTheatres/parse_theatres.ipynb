{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "# --- 1. –£–°–¢–ê–ù–û–í–ö–ê –ë–ò–ë–õ–ò–û–¢–ï–ö ---\n",
    "def install_libs():\n",
    "    try:\n",
    "        import playwright\n",
    "        import bs4\n",
    "    except ImportError:\n",
    "        print(\"‚è≥ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"playwright\", \"beautifulsoup4\", \"pandas\"])\n",
    "        os.system(\"playwright install chromium\")\n",
    "        os.system(\"playwright install-deps\")\n",
    "        print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≥–æ—Ç–æ–≤—ã.\")\n",
    "\n",
    "install_libs()\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ==========================================\n",
    "# –ß–ê–°–¢–¨ 1: –î–†–ê–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –¢–ï–ê–¢–† (DRAMTEATR)\n",
    "# ==========================================\n",
    "\n",
    "async def scrape_dram_schedule(page):\n",
    "    url = \"https://dramteatr39.ru/afisha\"\n",
    "    print(f\"\\nüé≠ [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –≠—Ç–∞–ø 1: –°–∫–∞–Ω–∏—Ä—É–µ–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ {url}...\")\n",
    "\n",
    "    try:\n",
    "        await page.goto(url, timeout=90000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(3000)\n",
    "\n",
    "        # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞\n",
    "        scrolls = 6\n",
    "        for _ in range(scrolls):\n",
    "            await page.mouse.wheel(0, 4000)\n",
    "            await page.wait_for_timeout(random.randint(1000, 3000))\n",
    "\n",
    "        content = await page.content()\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        events_list = []\n",
    "        event_links = soup.find_all('a', href=re.compile(r'/spektakli/'))\n",
    "        seen_identifiers = set()\n",
    "\n",
    "        for link in event_links:\n",
    "            href = link.get('href')\n",
    "            full_link = f\"https://dramteatr39.ru{href}\" if href.startswith('/') else href\n",
    "\n",
    "            title = link.get_text(strip=True)\n",
    "            if len(title) < 2:\n",
    "                continue\n",
    "\n",
    "            container = link.find_parent(class_='affiche__container')\n",
    "            card = container\n",
    "            date_block = None\n",
    "            if container:\n",
    "                date_block = container.select_one('.affiche__date-block')\n",
    "            else:\n",
    "                # Fallback for unexpected markup changes.\n",
    "                card = link.find_parent()\n",
    "                if card:\n",
    "                    date_block = card.select_one('.affiche__date-block')\n",
    "\n",
    "            if not card:\n",
    "                continue\n",
    "\n",
    "            card_text = card.get_text(\" \", strip=True).upper()\n",
    "\n",
    "            # –î–∞—Ç–∞\n",
    "            date_text = \"–î–∞—Ç–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\"\n",
    "            day_text = \"\"\n",
    "            month_text = \"\"\n",
    "            time_text = \"\"\n",
    "\n",
    "            if date_block:\n",
    "                day_node = date_block.select_one('.date-list__day')\n",
    "                month_node = date_block.select_one('.date-list__month')\n",
    "                time_node = date_block.select_one('.date-list__time')\n",
    "\n",
    "                if day_node:\n",
    "                    day_text = day_node.get_text(strip=True)\n",
    "                if month_node:\n",
    "                    month_text = month_node.get_text(strip=True)\n",
    "                if time_node:\n",
    "                    spans = [s.get_text(strip=True) for s in time_node.find_all('span') if s.get_text(strip=True)]\n",
    "                    if spans:\n",
    "                        time_text = spans[-1]\n",
    "                    else:\n",
    "                        raw_time = time_node.get_text(\" \", strip=True)\n",
    "                        parts = [p for p in raw_time.split() if \":\" in p]\n",
    "                        if parts:\n",
    "                            time_text = parts[-1]\n",
    "\n",
    "            if day_text and month_text:\n",
    "                date_text = f\"{day_text} {month_text}\"\n",
    "                if time_text:\n",
    "                    date_text = f\"{date_text} {time_text}\"\n",
    "\n",
    "            # –°—Ç–∞—Ç—É—Å\n",
    "            status = \"available\"\n",
    "            if \"–ë–ò–õ–ï–¢–û–í –ù–ï–¢\" in card_text or \"–ü–†–û–î–ê–ù–û\" in card_text:\n",
    "                status = \"sold_out\"\n",
    "\n",
    "            # –°—Ü–µ–Ω–∞\n",
    "            if \"–û–°–ù–û–í–ù–ê–Ø\" in card_text:\n",
    "                scene = \"–û—Å–Ω–æ–≤–Ω–∞—è\"\n",
    "            elif \"–°–†–ï–î–ù–Ø–Ø\" in card_text:\n",
    "                scene = \"–°—Ä–µ–¥–Ω—è—è\"\n",
    "            else:\n",
    "                scene = \"–ú–∞–ª–∞—è\"\n",
    "\n",
    "            event_id = f\"{date_text}_{title}\"\n",
    "            if event_id in seen_identifiers:\n",
    "                continue\n",
    "            seen_identifiers.add(event_id)\n",
    "\n",
    "            events_list.append({\n",
    "                \"title\": title,\n",
    "                \"date_raw\": date_text,\n",
    "                \"ticket_status\": status,\n",
    "                \"scene\": scene,\n",
    "                \"url\": full_link,\n",
    "                \"location\": \"–î—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ–∞—Ç—Ä\"\n",
    "            })\n",
    "\n",
    "        print(f\"‚úÖ [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events_list)}\")\n",
    "        return events_list\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è: {e}\")\n",
    "        return []\n",
    "\n",
    "async def scrape_dram_details(context, unique_urls):\n",
    "    print(f\"üïµÔ∏è‚Äç‚ôÇÔ∏è [–î—Ä–∞–º—Ç–µ–∞—Ç—Ä] –°–±–æ—Ä –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {len(unique_urls)} —Å–ø–µ–∫—Ç–∞–∫–ª–µ–π...\")\n",
    "    details_map = {}\n",
    "    page = await context.new_page()\n",
    "\n",
    "    for url in unique_urls:\n",
    "        try:\n",
    "            await page.goto(url, timeout=45000, wait_until='domcontentloaded')\n",
    "            try:\n",
    "                await page.wait_for_selector('h1', timeout=3000)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            content = await page.content()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            full_text = soup.get_text(\" \", strip=True).upper()\n",
    "\n",
    "            is_pushkin = \"–ü–£–®–ö–ò–ù–°–ö–ê–Ø –ö–ê–†–¢–ê\" in full_text\n",
    "\n",
    "            images = []\n",
    "            for a in soup.find_all('a', href=re.compile(r'\\.(jpg|jpeg|png)$', re.IGNORECASE)):\n",
    "                img_url = a['href']\n",
    "                if img_url.startswith('/'):\n",
    "                    img_url = f\"https://dramteatr39.ru{img_url}\"\n",
    "                if img_url not in images:\n",
    "                    images.append(img_url)\n",
    "\n",
    "            if len(images) < 2:\n",
    "                for img in soup.select('div.swiper-slide img, .gallery img, .content img'):\n",
    "                    src = img.get('src')\n",
    "                    if src and 'logo' not in src:\n",
    "                        if src.startswith('/'):\n",
    "                            src = f\"https://dramteatr39.ru{src}\"\n",
    "                        if src not in images:\n",
    "                            images.append(src)\n",
    "\n",
    "            desc_text = \"\"\n",
    "            desc_div = soup.find('div', class_=re.compile(r'description|text-block'))\n",
    "            if not desc_div:\n",
    "                header = soup.find(string=re.compile(\"–û —Å–ø–µ–∫—Ç–∞–∫–ª–µ\"))\n",
    "                if header and header.parent:\n",
    "                    parent = header.find_parent('div')\n",
    "                    if parent:\n",
    "                        desc_div = parent\n",
    "            if desc_div:\n",
    "                desc_text = desc_div.get_text(\"\\n\", strip=True)\n",
    "\n",
    "            creators = \"\"\n",
    "            c_div = soup.find('div', class_=re.compile(r'creators|team'))\n",
    "            if c_div:\n",
    "                creators = c_div.get_text(\" | \", strip=True)\n",
    "\n",
    "            details_map[url] = {\n",
    "                \"pushkin_card\": is_pushkin,\n",
    "                \"photos\": images,\n",
    "                \"description\": desc_text[:1500],\n",
    "                \"creators\": creators[:500]\n",
    "            }\n",
    "        except Exception:\n",
    "            details_map[url] = {}\n",
    "\n",
    "    await page.close()\n",
    "    return details_map\n",
    "\n",
    "async def run_dramteatr(browser):\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    page = await context.new_page()\n",
    "\n",
    "    schedule = await scrape_dram_schedule(page)\n",
    "    if not schedule:\n",
    "        await context.close()\n",
    "        return []\n",
    "\n",
    "    unique_links = list(set(x['url'] for x in schedule))\n",
    "    details = await scrape_dram_details(context, unique_links)\n",
    "\n",
    "    await context.close()\n",
    "\n",
    "    final_data = []\n",
    "    for item in schedule:\n",
    "        det = details.get(item['url'], {})\n",
    "        final_data.append({**item, **det})\n",
    "\n",
    "    return final_data\n",
    "\n",
    "# ==========================================\n",
    "# –ß–ê–°–¢–¨ 2: –ú–£–ó–´–ö–ê–õ–¨–ù–´–ô –¢–ï–ê–¢–† (MUZTEATR)\n",
    "# ==========================================\n",
    "\n",
    "BASE_URL_MUZ = \"https://muzteatr39.ru\"\n",
    "\n",
    "async def find_muz_afisha_link(page):\n",
    "    try:\n",
    "        await page.goto(BASE_URL_MUZ, timeout=30000, wait_until='domcontentloaded')\n",
    "        link = page.get_by_role(\"link\", name=re.compile(\"–ê—Ñ–∏—à–∞\", re.IGNORECASE)).first\n",
    "        href = await link.get_attribute(\"href\")\n",
    "        if href:\n",
    "            return f\"{BASE_URL_MUZ}{href}\" if href.startswith(\"/\") else href\n",
    "    except:\n",
    "        pass\n",
    "    return f\"{BASE_URL_MUZ}/afisha\"\n",
    "\n",
    "async def scrape_muz_schedule(page, afisha_url):\n",
    "    print(f\"\\nüéª [–ú—É–∑—Ç–µ–∞—Ç—Ä] –≠—Ç–∞–ø 1: –°–∫–∞–Ω–∏—Ä—É–µ–º –∞—Ñ–∏—à—É: {afisha_url}...\")\n",
    "    try:\n",
    "        await page.goto(afisha_url, timeout=60000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(2000)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # –°–∫—Ä–æ–ª–ª\n",
    "    try:\n",
    "        for _ in range(3):\n",
    "            await page.mouse.wheel(0, 4000)\n",
    "            await page.wait_for_timeout(1000)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    content = await page.content()\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    events_list = []\n",
    "    unique_ids = set()\n",
    "    MONTHS_MAP = {'–Ø–ù–í–ê–†': '–Ø–ù–í–ê–†–Ø', '–§–ï–í–†–ê–õ': '–§–ï–í–†–ê–õ–Ø', '–ú–ê–†–¢': '–ú–ê–†–¢–ê', '–ê–ü–†–ï–õ': '–ê–ü–†–ï–õ–Ø', '–ú–ê–ô': '–ú–ê–Ø', '–ò–Æ–ù': '–ò–Æ–ù–Ø', '–ò–Æ–õ': '–ò–Æ–õ–Ø', '–ê–í–ì–£–°–¢': '–ê–í–ì–£–°–¢–ê', '–°–ï–ù–¢–Ø–ë–†': '–°–ï–ù–¢–Ø–ë–†–Ø', '–û–ö–¢–Ø–ë–†': '–û–ö–¢–Ø–ë–†–Ø', '–ù–û–Ø–ë–†': '–ù–û–Ø–ë–†–Ø', '–î–ï–ö–ê–ë–†': '–î–ï–ö–ê–ë–†–Ø'}\n",
    "    last_seen_month = \"?\"\n",
    "\n",
    "    rows = soup.find_all('div', class_=re.compile(r'row\\s+afisha'))\n",
    "    for row in rows:\n",
    "        date_col = row.find('div', class_=re.compile(r'afisha_data'))\n",
    "        day_str, month_str, time_str = \"?\", last_seen_month, \"00:00\"\n",
    "\n",
    "        if date_col:\n",
    "            col_text = date_col.get_text(\" \", strip=True).upper()\n",
    "            day_div = date_col.find('div', class_='afisha_nom')\n",
    "            if day_div:\n",
    "                day_str = day_div.get_text(strip=True)\n",
    "            else:\n",
    "                day_match = re.search(r'^(\\d{1,2})', col_text)\n",
    "                if day_match:\n",
    "                    day_str = day_match.group(1)\n",
    "\n",
    "            for root, full_name in MONTHS_MAP.items():\n",
    "                if root in col_text:\n",
    "                    month_str = full_name\n",
    "                    last_seen_month = full_name\n",
    "                    break\n",
    "\n",
    "            time_match = re.search(r'(\\d{1,2}:\\d{2})', col_text)\n",
    "            if time_match:\n",
    "                time_str = time_match.group(1).zfill(5)\n",
    "\n",
    "        if not day_str.isdigit():\n",
    "            continue\n",
    "        date_final = f\"{day_str} {month_str} {time_str}\"\n",
    "\n",
    "        title_div = row.find('div', class_='afisha_spek_title')\n",
    "        if not title_div:\n",
    "            continue\n",
    "\n",
    "        title = \"\"\n",
    "        link_tag = title_div.find('a')\n",
    "        if link_tag:\n",
    "            title = link_tag.get_text(strip=True)\n",
    "            href = link_tag.get('href')\n",
    "        else:\n",
    "            badge = title_div.find('span', class_='badge')\n",
    "            if badge:\n",
    "                badge.decompose()\n",
    "            title = title_div.get_text(strip=True)\n",
    "            href = None\n",
    "\n",
    "        if not href:\n",
    "            continue\n",
    "        full_link = f\"{BASE_URL_MUZ}{href}\" if href.startswith('/') else href\n",
    "\n",
    "        badge = row.find('span', class_='badge')\n",
    "        age = badge.get_text(strip=True) if badge else \"\"\n",
    "\n",
    "        row_text_full = row.get_text(\" \", strip=True).upper()\n",
    "        status = \"available\"\n",
    "        if \"–ë–ò–õ–ï–¢–û–í –ù–ï–¢\" in row_text_full or \"–ü–†–û–î–ê–ù–û\" in row_text_full:\n",
    "            status = \"sold_out\"\n",
    "\n",
    "        eid = f\"{date_final}_{full_link}\"\n",
    "        if eid in unique_ids:\n",
    "            continue\n",
    "        unique_ids.add(eid)\n",
    "\n",
    "        events_list.append({\n",
    "            \"title\": title,\n",
    "            \"date_raw\": date_final,\n",
    "            \"age_restriction\": age,\n",
    "            \"ticket_status\": status,\n",
    "            \"url\": full_link,\n",
    "            \"location\": \"–ú—É–∑—ã–∫–∞–ª—å–Ω—ã–π —Ç–µ–∞—Ç—Ä\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ [–ú—É–∑—Ç–µ–∞—Ç—Ä] –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events_list)}\")\n",
    "    return events_list\n",
    "\n",
    "async def scrape_muz_details(context, unique_urls):\n",
    "    print(f\"üïµÔ∏è‚Äç‚ôÇÔ∏è [–ú—É–∑—Ç–µ–∞—Ç—Ä] –°–±–æ—Ä –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {len(unique_urls)} —Å–æ–±—ã—Ç–∏–π...\")\n",
    "    details = {}\n",
    "    page = await context.new_page()\n",
    "    BAD_IMAGES = ['ulogin', 'provider', 'icon', 'logo', 'social', 'vk.com', 'ok.ru', 'blank', 'pixel']\n",
    "\n",
    "    for url in unique_urls:\n",
    "        try:\n",
    "            if not url or 'http' not in url:\n",
    "                continue\n",
    "            try:\n",
    "                await page.goto(url, timeout=30000, wait_until='domcontentloaded')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            content = await page.content()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            desc_text = \"\"\n",
    "            meta_og = soup.find(\"meta\", property=\"og:description\")\n",
    "            if meta_og and meta_og.get(\"content\"):\n",
    "                desc_text = meta_og[\"content\"].strip()\n",
    "            if not desc_text:\n",
    "                meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "                if meta_desc and meta_desc.get(\"content\"):\n",
    "                    desc_text = meta_desc[\"content\"].strip()\n",
    "            if not desc_text:\n",
    "                content_div = soup.find('div', class_=re.compile(r'detail-text|news-detail|item-text'))\n",
    "                if content_div:\n",
    "                    raw = content_div.get_text(\"\\n\", strip=True)\n",
    "                    lines = [l for l in raw.split('\\n') if len(l) > 30 and '–≤–æ–π—Ç–∏' not in l.lower()]\n",
    "                    desc_text = \"\\n\".join(lines)[:2000]\n",
    "\n",
    "            images = []\n",
    "            for d in soup.find_all('div', style=re.compile(r'url\\(')):\n",
    "                match = re.search(r'url\\([\\'\\\"]?(.*?)[\\'\\\"]?\\)', d['style'])\n",
    "                if match:\n",
    "                    u = match.group(1)\n",
    "                    if u.startswith('/'):\n",
    "                        u = f\"{BASE_URL_MUZ}{u}\"\n",
    "                    if not any(b in u.lower() for b in BAD_IMAGES):\n",
    "                        images.append(u)\n",
    "\n",
    "            container = soup.find('div', class_=re.compile(r'container|main|content')) or soup\n",
    "            for img in container.find_all('img'):\n",
    "                src = img.get('src')\n",
    "                if src and not src.endswith('.svg') and not any(b in src.lower() for b in BAD_IMAGES):\n",
    "                    if src.startswith('/'):\n",
    "                        src = f\"{BASE_URL_MUZ}{src}\"\n",
    "                    images.append(src)\n",
    "\n",
    "            details[url] = {\n",
    "                \"pushkin_card\": \"–ü–£–®–ö–ò–ù–°–ö–ê–Ø –ö–ê–†–¢–ê\" in soup.get_text().upper(),\n",
    "                \"description\": desc_text,\n",
    "                \"photos\": list(set(images))\n",
    "            }\n",
    "        except:\n",
    "            details[url] = {}\n",
    "\n",
    "    await page.close()\n",
    "    return details\n",
    "\n",
    "async def run_muzteatr(browser):\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    page = await context.new_page()\n",
    "\n",
    "    afisha_url = await find_muz_afisha_link(page)\n",
    "    events = await scrape_muz_schedule(page, afisha_url)\n",
    "\n",
    "    if not events:\n",
    "        await context.close()\n",
    "        return []\n",
    "\n",
    "    urls = list(set(x['url'] for x in events))\n",
    "    details = await scrape_muz_details(context, urls)\n",
    "\n",
    "    await context.close()\n",
    "\n",
    "    final = []\n",
    "    for ev in events:\n",
    "        det = details.get(ev['url'], {})\n",
    "        final.append({**ev, **det})\n",
    "    return final\n",
    "\n",
    "BASE_URL_SOBOR = \"https://sobor39.ru\"\n",
    "\n",
    "# --- 2. –ü–ê–†–°–ï–† –°–û–ë–û–†–ê ---\n",
    "async def scrape_sobor_strict(page):\n",
    "    target_url = \"https://sobor39.ru/events/concerts/night/\"\n",
    "    print(f\"‚õ™ –ó–∞—Ö–æ–¥–∏–º –Ω–∞ {target_url}...\")\n",
    "\n",
    "    try:\n",
    "        await page.goto(target_url, timeout=90000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(5000)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏: {e}. –ü—Ä–æ–±—É–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–µ–º, —á—Ç–æ –µ—Å—Ç—å.\")\n",
    "\n",
    "    # --- –ü–†–û–ö–†–£–¢–ö–ê ---\n",
    "    print(\"‚¨áÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∫—É –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ —Å–æ–±—ã—Ç–∏–π...\")\n",
    "\n",
    "    for i in range(10):\n",
    "        await page.mouse.wheel(0, 5000)\n",
    "        await page.wait_for_timeout(3000)\n",
    "\n",
    "        try:\n",
    "            more_btns = page.locator(\"text=/–ü–æ–∫–∞–∑–∞—Ç—å –µ—â[—ë–µ]|–ó–∞–≥—Ä—É–∑–∏—Ç—å/i\")\n",
    "            if await more_btns.count() > 0:\n",
    "                if await more_btns.first.is_visible():\n",
    "                    await more_btns.first.click()\n",
    "                    await page.wait_for_timeout(2000)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            print(f\"   ...–ø—Ä–æ–∫—Ä—É—Ç–∫–∞ {i+1}/10\")\n",
    "\n",
    "    content = await page.content()\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    events_list = []\n",
    "    unique_keys = set()\n",
    "\n",
    "    items = soup.find_all('div', class_='list-item')\n",
    "    print(f\"üîç –ù–∞–π–¥–µ–Ω–æ –±–ª–æ–∫–æ–≤ .list-item: {len(items)}\")\n",
    "\n",
    "    for item in items:\n",
    "        try:\n",
    "            # 1. –ó–ê–ì–û–õ–û–í–û–ö\n",
    "            title_tag = item.find('div', class_='list-t')\n",
    "            if not title_tag:\n",
    "                continue\n",
    "            title = title_tag.get_text(strip=True)\n",
    "\n",
    "            # 2. –î–ê–¢–ê (–†–∞–∑–¥–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã)\n",
    "            date_raw = \"Unknown\"\n",
    "            day_node = item.find('div', class_='list-day')\n",
    "            month_node = item.find('div', class_='list-month')\n",
    "            time_node = item.find('div', class_='list-time')\n",
    "\n",
    "            if day_node and month_node:\n",
    "                d_txt = day_node.get_text(strip=True)\n",
    "                m_txt = month_node.get_text(strip=True)\n",
    "                t_txt = time_node.get_text(strip=True) if time_node else \"\"\n",
    "                date_raw = f\"{d_txt} {m_txt} {t_txt}\".strip()\n",
    "\n",
    "            # 3. –°–°–´–õ–ö–ê –ò –°–¢–ê–¢–£–°\n",
    "            url = \"\"\n",
    "            status = \"available\"\n",
    "\n",
    "            other_block = item.find('div', class_='list-other')\n",
    "            if other_block:\n",
    "                btn = other_block.find('a', href=True)\n",
    "                if btn:\n",
    "                    link = btn['href']\n",
    "                    url = link if link.startswith('http') else f\"{BASE_URL_SOBOR}{link}\"\n",
    "                    btn_text = btn.get_text(strip=True).upper()\n",
    "                    if \"SOLD\" in btn_text or \"–ü–†–û–î–ê–ù–û\" in btn_text:\n",
    "                        status = \"sold_out\"\n",
    "\n",
    "            if not url:\n",
    "                status = \"unknown\"\n",
    "\n",
    "            # 4. –ö–ê–†–¢–ò–ù–ö–ê\n",
    "            photo_url = \"\"\n",
    "            img_block = item.find('div', class_='list-img')\n",
    "            if img_block:\n",
    "                img = img_block.find('img')\n",
    "                if img and img.get('src'):\n",
    "                    src = img['src']\n",
    "                    if 'svg' not in src:\n",
    "                        photo_url = src if src.startswith('http') else f\"{BASE_URL_SOBOR}{src}\"\n",
    "\n",
    "            # 5. –û–ü–ò–°–ê–ù–ò–ï (Markdown)\n",
    "            description = \"\"\n",
    "            descr_node = item.find('div', class_='list-descr')\n",
    "            if descr_node:\n",
    "                # get_text(separator=\"\\n\") –∑–∞–º–µ–Ω—è–µ—Ç <br> –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Ç–µ–≥–∏ –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏\n",
    "                description = descr_node.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "            # –î–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—è\n",
    "            key = f\"{date_raw}_{title}\"\n",
    "            if key in unique_keys:\n",
    "                continue\n",
    "            unique_keys.add(key)\n",
    "\n",
    "            events_list.append({\n",
    "                \"title\": title,\n",
    "                \"date_raw\": date_raw,\n",
    "                \"ticket_status\": status,\n",
    "                \"url\": url,\n",
    "                \"photos\": [photo_url] if photo_url else [],\n",
    "                \"description\": description,\n",
    "                \"pushkin_card\": False,\n",
    "                \"location\": \"–ö–∞—Ñ–µ–¥—Ä–∞–ª—å–Ω—ã–π —Å–æ–±–æ—Ä\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events_list)}\")\n",
    "    return events_list\n",
    "\n",
    "async def run_sobor(browser):\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    page = await context.new_page()\n",
    "\n",
    "    data = await scrape_sobor_strict(page)\n",
    "    await context.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# –ß–ê–°–¢–¨ 3: –¢–†–ï–¢–¨–Ø–ö–û–í–°–ö–ê–Ø –ì–ê–õ–ï–†–ï–Ø\n",
    "# ==========================================\n",
    "\n",
    "BASE_URL_TRETYAKOV = \"https://kaliningrad.tretyakovgallery.ru\"\n",
    "MAX_EVENTS_TO_PROCESS = 1000  # Production limit\n",
    "MAX_DATE_TIME_COMBOS = 30\n",
    "\n",
    "MONTHS_RU = {\n",
    "    \"—è–Ω–≤–∞—Ä—è\": 1, \"—Ñ–µ–≤—Ä–∞–ª—è\": 2, \"–º–∞—Ä—Ç–∞\": 3, \"–∞–ø—Ä–µ–ª—è\": 4, \"–º–∞—è\": 5, \"–∏—é–Ω—è\": 6,\n",
    "    \"–∏—é–ª—è\": 7, \"–∞–≤–≥—É—Å—Ç–∞\": 8, \"—Å–µ–Ω—Ç—è–±—Ä—è\": 9, \"–æ–∫—Ç—è–±—Ä—è\": 10, \"–Ω–æ—è–±—Ä—è\": 11, \"–¥–µ–∫–∞–±—Ä—è\": 12\n",
    "}\n",
    "\n",
    "\n",
    "async def scrape_tretyakov_events_list(page):\n",
    "    \"\"\"Scrape events from /events/ page, extracting detail_url and ticket_url.\"\"\"\n",
    "    url = f\"{BASE_URL_TRETYAKOV}/events/\"\n",
    "    print(f\"\\nüñºÔ∏è [–¢—Ä–µ—Ç—å—è–∫–æ–≤–∫–∞] Scanning: {url}\")\n",
    "    \n",
    "    await page.goto(url, timeout=60000, wait_until='domcontentloaded')\n",
    "    await page.wait_for_timeout(3000)\n",
    "    \n",
    "    for _ in range(3):\n",
    "        await page.mouse.wheel(0, 3000)\n",
    "        await page.wait_for_timeout(random.randint(1000, 1500))\n",
    "    \n",
    "    events = await page.evaluate(\"\"\"\n",
    "        () => {\n",
    "            const events = [];\n",
    "            const seen = new Set();\n",
    "            const BASE = 'https://kaliningrad.tretyakovgallery.ru';\n",
    "            \n",
    "            document.querySelectorAll('.card').forEach(card => {\n",
    "                const titleEl = card.querySelector('.card_title');\n",
    "                if (!titleEl) return;\n",
    "                \n",
    "                const title = titleEl.innerText.trim();\n",
    "                if (title.toUpperCase().includes('–≠–ö–°–ö–£–†–°–ò–Ø')) return;\n",
    "                \n",
    "                // Get detail_url from onclick\n",
    "                let detailUrl = null;\n",
    "                const onclick = card.getAttribute('onclick');\n",
    "                if (onclick) {\n",
    "                    const match = onclick.match(/window\\\\.open\\\\(['\"]([^'\"]+)['\"]/);\n",
    "                    if (match) detailUrl = match[1];\n",
    "                }\n",
    "                \n",
    "                // Get ticket_url\n",
    "                let ticketUrl = null;\n",
    "                const ticketLink = card.querySelector('a[href*=\"tickets\"]');\n",
    "                if (ticketLink) {\n",
    "                    let href = ticketLink.getAttribute('href');\n",
    "                    if (href.startsWith('//')) ticketUrl = 'https:' + href;\n",
    "                    else ticketUrl = href;\n",
    "                }\n",
    "                \n",
    "                if (ticketUrl && ticketUrl.includes('timepad')) return;\n",
    "                \n",
    "                // Get photo\n",
    "                let photo = null;\n",
    "                const img = card.querySelector('img.card_img');\n",
    "                if (img && img.src) {\n",
    "                    photo = img.src.startsWith('/') ? BASE + img.src : img.src;\n",
    "                }\n",
    "                \n",
    "                // Get location\n",
    "                const text = card.innerText.toUpperCase();\n",
    "                let location = '–¢—Ä–µ—Ç—å—è–∫–æ–≤–∫–∞ –ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥';\n",
    "                if (text.includes('–ê–¢–†–ò–£–ú')) location = '–ê—Ç—Ä–∏—É–º';\n",
    "                else if (text.includes('–ö–ò–ù–û–ó–ê–õ')) location = '–ö–∏–Ω–æ–∑–∞–ª';\n",
    "                \n",
    "                const key = title + ticketUrl;\n",
    "                if (seen.has(key)) return;\n",
    "                seen.add(key);\n",
    "                \n",
    "                if (ticketUrl) {\n",
    "                    events.push({\n",
    "                        title_raw: title,\n",
    "                        detail_url: detailUrl,\n",
    "                        ticket_url: ticketUrl,\n",
    "                        photo: photo,\n",
    "                        location: location\n",
    "                    });\n",
    "                }\n",
    "            });\n",
    "            return events;\n",
    "        }\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Found {len(events)} events\")\n",
    "    return events[:MAX_EVENTS_TO_PROCESS]\n",
    "\n",
    "\n",
    "async def scrape_tretyakov_detail(page, detail_url):\n",
    "    \"\"\"Visit detail page for title and description.\"\"\"\n",
    "    if not detail_url:\n",
    "        return {\"title\": None, \"description\": None}\n",
    "    \n",
    "    full_url = f\"{BASE_URL_TRETYAKOV}{detail_url}\" if detail_url.startswith('/') else detail_url\n",
    "    print(f\"   üìÑ Detail: {full_url}\")\n",
    "    \n",
    "    try:\n",
    "        await page.goto(full_url, timeout=30000, wait_until='domcontentloaded')\n",
    "        await page.wait_for_timeout(2000)\n",
    "        \n",
    "        title = None\n",
    "        h1 = await page.query_selector('h1')\n",
    "        if h1:\n",
    "            title = (await h1.inner_text()).strip()\n",
    "        \n",
    "        description = None\n",
    "        paragraphs = await page.query_selector_all('p')\n",
    "        for p in paragraphs:\n",
    "            text = (await p.inner_text()).strip()\n",
    "            if len(text) > 80 and not any(skip in text.lower() for skip in ['cookie', '–ø–æ–ª–∏—Ç–∏–∫', 'hours']):\n",
    "                description = text[:500]\n",
    "                break\n",
    "        \n",
    "        return {\"title\": title, \"description\": description}\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è Detail error: {e}\")\n",
    "        return {\"title\": None, \"description\": None}\n",
    "\n",
    "\n",
    "async def scrape_tretyakov_tickets(page, ticket_url):\n",
    "    \"\"\"Parse ticket page for dates, times, prices. Only clicks ACTIVE dates.\"\"\"\n",
    "    import datetime\n",
    "    \n",
    "    full_url = f\"{BASE_URL_TRETYAKOV}{ticket_url}\" if ticket_url.startswith('/') else ticket_url\n",
    "    print(f\"   üé´ Tickets: {full_url[:60]}...\")\n",
    "    today = datetime.date.today()\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        await page.goto(full_url, timeout=60000, wait_until='networkidle')\n",
    "        await page.wait_for_timeout(3000)\n",
    "        \n",
    "        # Get ACTIVE dates only\n",
    "        active_dates = await page.evaluate(\"\"\"\n",
    "            () => {\n",
    "                const items = [];\n",
    "                document.querySelectorAll('div.item.active').forEach(item => {\n",
    "                    const dayEl = item.querySelector('.calendarDay');\n",
    "                    const monthEl = item.querySelector('.calendarMonth');\n",
    "                    if (dayEl) {\n",
    "                        items.push({ \n",
    "                            day: dayEl.innerText.trim(), \n",
    "                            month: monthEl ? monthEl.innerText.trim().toLowerCase() : '' \n",
    "                        });\n",
    "                    }\n",
    "                });\n",
    "                return items;\n",
    "            }\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"      üìÖ ACTIVE dates: {len(active_dates)}\")\n",
    "        \n",
    "        for d in active_dates:\n",
    "            if len(results) >= MAX_DATE_TIME_COMBOS:\n",
    "                break\n",
    "            \n",
    "            month_num = MONTHS_RU.get(d['month'], 1)\n",
    "            year = today.year + (1 if today.month >= 10 and month_num < 3 else 0)\n",
    "            \n",
    "            try:\n",
    "                date_obj = datetime.date(year, month_num, int(d['day']))\n",
    "                if date_obj < today:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            date_iso = date_obj.isoformat()\n",
    "            date_raw = f\"{d['day']} {d['month']}\"\n",
    "            \n",
    "            # Click date\n",
    "            day_val = d['day']\n",
    "            await page.evaluate(f\"() => {{ document.querySelectorAll('div.item.active').forEach(i => {{ const dayEl = i.querySelector('.calendarDay'); if (dayEl && dayEl.innerText.trim() === '{day_val}') i.click(); }}); }}\")\n",
    "            await page.wait_for_timeout(1500)\n",
    "            \n",
    "            # Get times\n",
    "            times = await page.evaluate(\"\"\"() => [...document.querySelectorAll('label.select-time-button:not(.disabled)')].map(b => b.innerText.trim().match(/^\\\\d{1,2}:\\\\d{2}$/)?.[0]).filter(Boolean)\"\"\")\n",
    "            \n",
    "            if not times:\n",
    "                times = ['00:00']\n",
    "            \n",
    "            print(f\"         {date_raw}: {len(times)} times\")\n",
    "            \n",
    "            for time_str in times:\n",
    "                if len(results) >= MAX_DATE_TIME_COMBOS:\n",
    "                    break\n",
    "                \n",
    "                # Click time\n",
    "                await page.evaluate(f\"() => {{ document.querySelectorAll('label.select-time-button').forEach(b => {{ if (b.innerText.includes('{time_str}')) b.click(); }}); }}\")\n",
    "                await page.wait_for_timeout(1500)\n",
    "                \n",
    "                # Click sector if present\n",
    "                sectors = await page.locator('text=/[–°—Å]–µ–∫—Ç–æ—Ä/').all()\n",
    "                if sectors:\n",
    "                    try:\n",
    "                        await sectors[0].click()\n",
    "                        await page.wait_for_timeout(1000)\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Extract price\n",
    "                prices = await page.evaluate(\"\"\"() => [...new Set([...document.querySelectorAll('*')].map(e => e.innerText?.match(/(\\\\d+)\\\\s*‚ÇΩ/)?.[1]).filter(Boolean).map(Number).filter(p => p > 100))]\"\"\")\n",
    "                price = min(prices) if prices else None\n",
    "                status = \"available\" if prices else \"unknown\"\n",
    "                \n",
    "                body = await page.inner_text(\"body\")\n",
    "                if \"–≤—Å–µ –±–∏–ª–µ—Ç—ã –ø—Ä–æ–¥–∞–Ω—ã\" in body.lower():\n",
    "                    status = \"sold_out\"\n",
    "                \n",
    "                results.append({\n",
    "                    \"parsed_date\": date_iso,\n",
    "                    \"parsed_time\": time_str,\n",
    "                    \"date_raw\": f\"{date_raw} –≤ {time_str}\",\n",
    "                    \"ticket_price_min\": price,\n",
    "                    \"ticket_price_max\": price,\n",
    "                    \"ticket_status\": status,\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è Ticket error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "async def run_tretyakov(browser):\n",
    "    \"\"\"Main parser with proven working algorithm.\"\"\"\n",
    "    context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n",
    "    await context.route(\"**/*{google,yandex,metrika,analytics}*\", lambda route: route.abort())\n",
    "    \n",
    "    list_page = await context.new_page()\n",
    "    detail_page = await context.new_page()\n",
    "    ticket_page = await context.new_page()\n",
    "\n",
    "    events_raw = await scrape_tretyakov_events_list(list_page)\n",
    "    if not events_raw:\n",
    "        await context.close()\n",
    "        return []\n",
    "\n",
    "    all_events = []\n",
    "    \n",
    "    for idx, event in enumerate(events_raw):\n",
    "        print(f\"\\nüìå [{idx+1}/{len(events_raw)}] {event['title_raw'][:50]}...\")\n",
    "        \n",
    "        # Clean ticket_url\n",
    "        raw_url = event['ticket_url']\n",
    "        # 1. Remove absolute prefix if present\n",
    "        if raw_url.startswith(BASE_URL_TRETYAKOV):\n",
    "            raw_url = raw_url[len(BASE_URL_TRETYAKOV):]\n",
    "        elif raw_url.startswith('http'):\n",
    "            # External or other domain, keep as is but careful with base concatenation\n",
    "            pass\n",
    "            \n",
    "        # 2. Remove trailing date/time components (e.g. /2026-01-07/20:00:00)\n",
    "        # Regex for /YYYY-MM-DD/HH:MM:SS or /YYYY-MM-DD/HH:MM\n",
    "        import re\n",
    "        clean_url = re.sub(r'/\\d{4}-\\d{2}-\\d{2}/\\d{2}:\\d{2}(:\\d{2})?$', '', raw_url)\n",
    "        clean_url = re.sub(r'/\\d{4}-\\d{2}-\\d{2}/\\d{2}:\\d{2}(:\\d{2})?$', '', clean_url) # Safety repeat\n",
    "        \n",
    "        # Use cleaned URL for processing\n",
    "        print(f\"   üîó Url: {clean_url}\")\n",
    "        \n",
    "        # Get title and description from detail page\n",
    "        detail = await scrape_tretyakov_detail(detail_page, event.get('detail_url'))\n",
    "        title = detail['title'] or event['title_raw']\n",
    "        description = detail['description']\n",
    "        \n",
    "        # Get dates/times/prices from ticket page\n",
    "        entries = await scrape_tretyakov_tickets(ticket_page, clean_url)\n",
    "        \n",
    "        photo = event['photo']\n",
    "        if photo and photo.startswith('/'):\n",
    "            photo = f\"{BASE_URL_TRETYAKOV}{photo}\"\n",
    "        \n",
    "        if not entries:\n",
    "            target_url = clean_url\n",
    "            if target_url.startswith('/'):\n",
    "                target_url = f\"{BASE_URL_TRETYAKOV}{target_url}\"\n",
    "                \n",
    "            all_events.append({\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"date_raw\": \"\",\n",
    "                \"parsed_date\": None,\n",
    "                \"parsed_time\": None,\n",
    "                \"ticket_status\": \"unknown\",\n",
    "                \"ticket_price_min\": None,\n",
    "                \"ticket_price_max\": None,\n",
    "                \"url\": target_url,\n",
    "                \"photos\": [photo] if photo else [],\n",
    "                \"location\": event['location'],\n",
    "                \"scene\": event['location'] if event['location'] in [\"–ê—Ç—Ä–∏—É–º\", \"–ö–∏–Ω–æ–∑–∞–ª\"] else \"\"\n",
    "            })\n",
    "        else:\n",
    "            for e in entries:\n",
    "                # Construct clean direct URL\n",
    "                base = clean_url\n",
    "                if base.startswith('/'):\n",
    "                    base = f\"{BASE_URL_TRETYAKOV}{base}\"\n",
    "                \n",
    "                direct_url = f\"{base}/{e['parsed_date']}/{e['parsed_time']}:00\"\n",
    "                all_events.append({\n",
    "                    \"title\": title,\n",
    "                    \"description\": description,\n",
    "                    \"date_raw\": e['date_raw'],\n",
    "                    \"parsed_date\": e['parsed_date'],\n",
    "                    \"parsed_time\": e['parsed_time'],\n",
    "                    \"ticket_status\": e['ticket_status'],\n",
    "                    \"ticket_price_min\": e['ticket_price_min'],\n",
    "                    \"ticket_price_max\": e['ticket_price_max'],\n",
    "                    \"url\": direct_url,\n",
    "                    \"photos\": [photo] if photo else [],\n",
    "                    \"location\": event['location'],\n",
    "                    \"scene\": event['location'] if event['location'] in [\"–ê—Ç—Ä–∏—É–º\", \"–ö–∏–Ω–æ–∑–∞–ª\"] else \"\"\n",
    "                })\n",
    "\n",
    "    print(f\"\\nüéâ [–¢—Ä–µ—Ç—å—è–∫–æ–≤–∫–∞] Total: {len(all_events)} event entries\")\n",
    "    await context.close()\n",
    "    return all_events\n",
    "\n",
    "\n",
    "# --- –ó–ê–ü–£–°–ö ---\n",
    "async def main():\n",
    "    config = {}\n",
    "    target = \"all\"\n",
    "    if os.path.exists('run_config.json'):\n",
    "        config = json.load(open('run_config.json'))\n",
    "        target = config.get('target_source', 'all')\n",
    "\n",
    "    print(\"üöÄ –ó–ê–ü–£–°–ö –°–ë–û–†–ê –î–ê–ù–ù–´–•...\")\n",
    "    print(\"‚ÑπÔ∏è [INFO] –ó–∞–ø—É—Å–∫–∞–µ–º –î—Ä–∞–º—Ç–µ–∞—Ç—Ä, –ú—É–∑—Ç–µ–∞—Ç—Ä, –ö–∞—Ñ–µ–¥—Ä–∞–ª—å–Ω—ã–π —Å–æ–±–æ—Ä –∏ –¢—Ä–µ—Ç—å—è–∫–æ–≤—Å–∫—É—é –≥–∞–ª–µ—Ä–µ—é.\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "\n",
    "        data_dram = []\n",
    "        data_muz = []\n",
    "        data_sobor = []\n",
    "        data_tretyakov = []\n",
    "\n",
    "        if target in (\"all\", \"dramteatr\"):\n",
    "            data_dram = await run_dramteatr(browser)\n",
    "            results[\"dramteatr\"] = data_dram\n",
    "            if data_dram:\n",
    "                with open('dramteatr.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(data_dram, f, ensure_ascii=False, indent=4)\n",
    "                pd.DataFrame(data_dram).to_csv('dramteatr.csv', index=False)\n",
    "                print(f\"üíæ Dramteatr saved: {len(data_dram)}\")\n",
    "\n",
    "        if target in (\"all\", \"muzteatr\"):\n",
    "            data_muz = await run_muzteatr(browser)\n",
    "            results[\"muzteatr\"] = data_muz\n",
    "            if data_muz:\n",
    "                with open('muzteatr.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(data_muz, f, ensure_ascii=False, indent=4)\n",
    "                df_muz = pd.DataFrame(data_muz)\n",
    "                if 'photos' in df_muz.columns:\n",
    "                    df_muz['photos'] = df_muz['photos'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else x)\n",
    "                df_muz.to_csv('muzteatr.csv', index=False)\n",
    "                print(f\"üíæ Muzteatr saved: {len(data_muz)}\")\n",
    "\n",
    "        if target in (\"all\", \"sobor\"):\n",
    "            data_sobor = await run_sobor(browser)\n",
    "            results[\"sobor\"] = data_sobor\n",
    "\n",
    "        if target in (\"all\", \"tretyakov\"):\n",
    "            data_tretyakov = await run_tretyakov(browser)\n",
    "            results[\"tretyakov\"] = data_tretyakov\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    all_data = []\n",
    "    for chunk in results.values():\n",
    "        if chunk:\n",
    "            all_data.extend(chunk)\n",
    "\n",
    "    if all_data:\n",
    "        results[\"all\"] = all_data\n",
    "\n",
    "    return results\n",
    "\n",
    "result = await main()\n",
    "\n",
    "sobor_data = result.get(\"sobor\", []) if result else []\n",
    "tretyakov_data = result.get(\"tretyakov\", []) if result else []\n",
    "all_data = result.get(\"all\", []) if result else []\n",
    "\n",
    "if sobor_data:\n",
    "    with open('sobor.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(sobor_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    df = pd.DataFrame(sobor_data)\n",
    "    if 'photos' in df.columns:\n",
    "        df['photo_preview'] = df['photos'].apply(lambda x: x[0] if x else \"\")\n",
    "\n",
    "    # –û–±—Ä–µ–∑–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ø—Ä–µ–≤—å—é –≤ –∫–æ–Ω—Å–æ–ª–∏\n",
    "    if 'description' in df.columns:\n",
    "        df['desc_preview'] = df['description'].apply(lambda x: (x[:50] + '...') if len(x) > 50 else x)\n",
    "\n",
    "    df.drop(columns=['photos', 'desc_preview'], inplace=True, errors='ignore')\n",
    "    df.to_csv('sobor.csv', index=False)\n",
    "\n",
    "if tretyakov_data:\n",
    "    with open('tretyakov.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(tretyakov_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    df_tretyakov = pd.DataFrame(tretyakov_data)\n",
    "    if 'photos' in df_tretyakov.columns:\n",
    "        df_tretyakov['photos'] = df_tretyakov['photos'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else x)\n",
    "    df_tretyakov.to_csv('tretyakov.csv', index=False)\n",
    "\n",
    "if all_data:\n",
    "    df_all = pd.DataFrame(all_data)\n",
    "    if 'photos' in df_all.columns:\n",
    "        df_all['photo_preview'] = df_all['photos'].apply(lambda x: x[0] if isinstance(x, list) and x else \"\")\n",
    "    if 'description' in df_all.columns:\n",
    "        df_all['desc_preview'] = df_all['description'].apply(lambda x: (x[:50] + '...') if len(x) > 50 else x)\n",
    "    else:\n",
    "        df_all['desc_preview'] = \"\"\n",
    "\n",
    "    print(f\"\\nüéâ –ò–¢–û–ì: {len(all_data)} —Å–æ–±—ã—Ç–∏–π.\")\n",
    "    pd.set_option('display.max_colwidth', 40)\n",
    "    print(df_all[['date_raw', 'title', 'desc_preview']].head(10))\n",
    "else:\n",
    "    print(\"–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\")\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}