{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegram Monitor\n",
    "\n",
    "Kaggle notebook for scanning Telegram sources and exporting `telegram_results.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q telethon google-generativeai requests pillow imagehash python-dotenv cryptography supabase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import hashlib\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import statistics\n",
    "import time\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "import google.generativeai as genai\n",
    "from telethon import TelegramClient\n",
    "from telethon.sessions import StringSession\n",
    "from telethon.tl.types import Channel, Chat\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "logger = logging.getLogger('telegram_monitor')\n",
    "import telethon\n",
    "logger.info('tg_monitor.telethon version=%s', getattr(telethon, '__version__', 'unknown'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_INPUT = Path('/kaggle/input')\n",
    "\n",
    "def _find_file(filename: str) -> Path | None:\n",
    "    if not KAGGLE_INPUT.exists():\n",
    "        return None\n",
    "    for path in KAGGLE_INPUT.rglob(filename):\n",
    "        if path.is_file():\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "def _load_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text(encoding='utf-8'))\n",
    "\n",
    "def load_config() -> dict:\n",
    "    path = _find_file('config.json')\n",
    "    if not path:\n",
    "        raise RuntimeError('config.json not found in /kaggle/input')\n",
    "    return _load_json(path)\n",
    "\n",
    "def load_secrets() -> dict:\n",
    "    enc_path = _find_file('secrets.enc')\n",
    "    key_path = _find_file('fernet.key')\n",
    "    if not enc_path or not key_path:\n",
    "        raise RuntimeError('secrets.enc/fernet.key not found in /kaggle/input')\n",
    "    from cryptography.fernet import Fernet\n",
    "    fernet = Fernet(key_path.read_bytes().strip())\n",
    "    decrypted = fernet.decrypt(enc_path.read_bytes())\n",
    "    return json.loads(decrypted.decode('utf-8'))\n",
    "\n",
    "config = load_config()\n",
    "secrets = load_secrets()\n",
    "for k, v in (secrets or {}).items():\n",
    "    if k and v and not os.getenv(k):\n",
    "        os.environ[k] = str(v)\n",
    "\n",
    "TG_SESSION = os.getenv('TG_SESSION', '')\n",
    "TG_API_ID = os.getenv('TG_API_ID', '')\n",
    "TG_API_HASH = os.getenv('TG_API_HASH', '')\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', '')\n",
    "\n",
    "if not TG_SESSION or not TG_API_ID or not TG_API_HASH:\n",
    "    raise RuntimeError('Missing TG credentials after secrets load')\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise RuntimeError('Missing GOOGLE_API_KEY after secrets load')\n",
    "\n",
    "logger.info(\n    'tg_monitor.secrets tg_session_len=%s tg_api_id_set=%s tg_api_hash_set=%s google_api_key_set=%s',\n    len(TG_SESSION) if TG_SESSION else 0,\n    bool(TG_API_ID),\n    bool(TG_API_HASH),\n    bool(GOOGLE_API_KEY),\n)\n",
    "\n",
    "# Gemma models\n",
    "TEXT_MODEL = os.getenv('TG_MONITORING_TEXT_MODEL', 'gemma-3-27b-it')\n",
    "VISION_MODEL = os.getenv('TG_MONITORING_VISION_MODEL', TEXT_MODEL)\n",
    "FALLBACK_TEXT_MODEL = os.getenv('TG_MONITORING_TEXT_MODEL_FALLBACK', TEXT_MODEL)\n",
    "FALLBACK_VISION_MODEL = os.getenv('TG_MONITORING_VISION_MODEL_FALLBACK', VISION_MODEL)\n",
    "\n",
    "# Scan limits\n",
    "MAX_MESSAGES_PER_SOURCE = int(os.getenv('TG_MONITORING_LIMIT', '50'))\n",
    "MAX_DAYS_BACK = int(os.getenv('TG_MONITORING_DAYS_BACK', '3'))\n",
    "MAX_IMAGES_PER_MESSAGE = int(os.getenv('TG_MONITORING_MAX_IMAGES', '4'))\n",
    "ENABLE_OCR = os.getenv('TG_MONITORING_ENABLE_OCR', '1') == '1'\n",
    "\n",
    "# Human-like delays\n",
    "HUMAN_DELAY_MIN = float(os.getenv('TG_MONITORING_DELAY_MIN', '0.8'))\n",
    "HUMAN_DELAY_MAX = float(os.getenv('TG_MONITORING_DELAY_MAX', '2.2'))\n",
    "HUMAN_LONG_PAUSE_EVERY = int(os.getenv('TG_MONITORING_LONG_PAUSE_EVERY', '7'))\n",
    "HUMAN_LONG_PAUSE_MIN = float(os.getenv('TG_MONITORING_LONG_PAUSE_MIN', '4'))\n",
    "HUMAN_LONG_PAUSE_MAX = float(os.getenv('TG_MONITORING_LONG_PAUSE_MAX', '9'))\n",
    "SOURCE_PAUSE_MIN = float(os.getenv('TG_MONITORING_SOURCE_PAUSE_MIN', '2'))\n",
    "SOURCE_PAUSE_MAX = float(os.getenv('TG_MONITORING_SOURCE_PAUSE_MAX', '6'))\n",
    "\n",
    "# Gemma rate limits (single limiter for all requests)\n",
    "RATE_RPM = int(os.getenv('TG_GEMMA_RPM', '30'))\n",
    "RATE_TPM = int(os.getenv('TG_GEMMA_TPM', '15000'))\n",
    "RATE_RPD = int(os.getenv('TG_GEMMA_RPD', '14400'))\n",
    "RATE_MINUTE_MARGIN = float(os.getenv('TG_GEMMA_MINUTE_MARGIN', '0.45'))\n",
    "RATE_DAILY_MARGIN = float(os.getenv('TG_GEMMA_DAILY_MARGIN', '0.85'))\n",
    "\n",
    "logger.info(\n",
    "    'tg_monitor.config sources=%d run_id=%s',\n",
    "    len(config.get('sources') or []),\n",
    "    config.get('run_id') or 'auto',\n",
    ")\n",
    "logger.info(\n",
    "    'tg_monitor.limits max_messages=%d max_days_back=%d max_images=%d ocr=%s',\n",
    "    MAX_MESSAGES_PER_SOURCE,\n",
    "    MAX_DAYS_BACK,\n",
    "    MAX_IMAGES_PER_MESSAGE,\n",
    "    ENABLE_OCR,\n",
    ")\n",
    "for src in config.get('sources') or []:\n",
    "    logger.info(\n",
    "        'tg_monitor.source_config username=%s last_id=%s default_location=%s trust_level=%s',\n",
    "        src.get('username'),\n",
    "        src.get('last_scanned_message_id'),\n",
    "        src.get('default_location'),\n",
    "        src.get('trust_level'),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RateLimitConfig:\n",
    "    rpm: int = RATE_RPM\n",
    "    tpm: int = RATE_TPM\n",
    "    rpd: int = RATE_RPD\n",
    "    minute_margin: float = RATE_MINUTE_MARGIN\n",
    "    daily_margin: float = RATE_DAILY_MARGIN\n",
    "\n",
    "    @property\n",
    "    def effective_rpm(self) -> int:\n",
    "        return int(self.rpm * (1 - self.minute_margin))\n",
    "\n",
    "    @property\n",
    "    def effective_tpm(self) -> int:\n",
    "        return int(self.tpm * (1 - self.minute_margin))\n",
    "\n",
    "    @property\n",
    "    def effective_rpd(self) -> int:\n",
    "        return int(self.rpd * (1 - self.daily_margin))\n",
    "\n",
    "\n",
    "class TokenBucket:\n",
    "    def __init__(self, capacity: int, refill_rate: float):\n",
    "        self.capacity = capacity\n",
    "        self.refill_rate = refill_rate\n",
    "        self.tokens = capacity\n",
    "        self.last_refill = time.monotonic()\n",
    "\n",
    "    def _refill(self) -> None:\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.last_refill\n",
    "        self.tokens = min(self.capacity, self.tokens + elapsed * self.refill_rate)\n",
    "        self.last_refill = now\n",
    "\n",
    "    def consume(self, tokens: int = 1) -> bool:\n",
    "        self._refill()\n",
    "        if self.tokens >= tokens:\n",
    "            self.tokens -= tokens\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def wait_time(self, tokens: int = 1) -> float:\n",
    "        self._refill()\n",
    "        if self.tokens >= tokens:\n",
    "            return 0.0\n",
    "        needed = tokens - self.tokens\n",
    "        return needed / self.refill_rate\n",
    "\n",
    "\n",
    "class GemmaRateLimiter:\n",
    "    def __init__(self, config: RateLimitConfig | None = None):\n",
    "        self.config = config or RateLimitConfig()\n",
    "        self._rpm_bucket = TokenBucket(\n",
    "            capacity=self.config.effective_rpm,\n",
    "            refill_rate=self.config.effective_rpm / 60.0,\n",
    "        )\n",
    "        self._tpm_bucket = TokenBucket(\n",
    "            capacity=self.config.effective_tpm,\n",
    "            refill_rate=self.config.effective_tpm / 60.0,\n",
    "        )\n",
    "        self._daily_requests = 0\n",
    "        self._last_reset_day: str | None = None\n",
    "\n",
    "    def _check_daily_reset(self) -> None:\n",
    "        today = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
    "        if self._last_reset_day != today:\n",
    "            self._daily_requests = 0\n",
    "            self._last_reset_day = today\n",
    "\n",
    "    async def wait_if_needed(self, estimated_tokens: int) -> None:\n",
    "        self._check_daily_reset()\n",
    "        if self._daily_requests >= self.config.effective_rpd:\n",
    "            logger.warning('Gemma daily request limit reached: %s', self.config.effective_rpd)\n",
    "        while True:\n",
    "            rpm_wait = self._rpm_bucket.wait_time(1)\n",
    "            if rpm_wait <= 0:\n",
    "                break\n",
    "            await asyncio.sleep(min(rpm_wait, 5.0))\n",
    "        while True:\n",
    "            tpm_wait = self._tpm_bucket.wait_time(estimated_tokens)\n",
    "            if tpm_wait <= 0:\n",
    "                break\n",
    "            await asyncio.sleep(min(tpm_wait, 5.0))\n",
    "        self._rpm_bucket.consume(1)\n",
    "        self._tpm_bucket.consume(estimated_tokens)\n",
    "        self._daily_requests += 1\n",
    "\n",
    "    def acquire(self, estimated_tokens: int = 500):\n",
    "        return RateLimitContext(self, estimated_tokens)\n",
    "\n",
    "\n",
    "class RateLimitContext:\n",
    "    def __init__(self, limiter: GemmaRateLimiter, estimated_tokens: int):\n",
    "        self._limiter = limiter\n",
    "        self._estimated_tokens = estimated_tokens\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        await self._limiter.wait_if_needed(self._estimated_tokens)\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        return False\n",
    "\n",
    "\n",
    "rate_limiter = GemmaRateLimiter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def human_sleep(min_s: float, max_s: float) -> None:\n",
    "    delay = random.uniform(min_s, max_s)\n",
    "    if random.random() < 0.12:\n",
    "        delay += random.uniform(0.8, 2.5)\n",
    "    await asyncio.sleep(delay)\n",
    "\n",
    "\n",
    "def _estimate_tokens(text: str, has_images: bool = False) -> int:\n",
    "    if not text:\n",
    "        return 200\n",
    "    base = max(200, len(text) // 4)\n",
    "    if has_images:\n",
    "        base += 800\n",
    "    return base\n",
    "\n",
    "\n",
    "def _safe_json(text: str):\n",
    "    if not text:\n",
    "        return None\n",
    "    raw = text.strip()\n",
    "    if raw.startswith('```'):\n",
    "        raw = re.sub(r'^```[a-zA-Z]*\\n?', '', raw).strip()\n",
    "        if raw.endswith('```'):\n",
    "            raw = raw[:-3].strip()\n",
    "    start = min([i for i in [raw.find('{'), raw.find('[')] if i != -1] or [-1])\n",
    "    end = max(raw.rfind('}'), raw.rfind(']'))\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        raw = raw[start:end+1]\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _is_not_found(exc: Exception) -> bool:\n",
    "    msg = str(exc).lower()\n",
    "    return 'not found' in msg or '404' in msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "MODEL_REGISTRY = {\n",
    "    'text': {\n",
    "        'name': TEXT_MODEL,\n",
    "        'fallback': FALLBACK_TEXT_MODEL,\n",
    "        'model': genai.GenerativeModel(TEXT_MODEL),\n",
    "    },\n",
    "    'vision': {\n",
    "        'name': VISION_MODEL,\n",
    "        'fallback': FALLBACK_VISION_MODEL,\n",
    "        'model': genai.GenerativeModel(VISION_MODEL),\n",
    "    },\n",
    "}\n",
    "\n",
    "SUPABASE_URL = os.getenv('SUPABASE_URL', '').strip()\n",
    "SUPABASE_SERVICE_KEY = os.getenv('SUPABASE_SERVICE_KEY', '').strip()\n",
    "SUPABASE_KEY = (SUPABASE_SERVICE_KEY or os.getenv('SUPABASE_KEY', '')).strip()\n",
    "SUPABASE_SCHEMA = (os.getenv('SUPABASE_SCHEMA', 'public') or 'public').strip()\n",
    "SUPABASE_ENABLED = bool(SUPABASE_URL and SUPABASE_KEY)\n",
    "SUPABASE_STRICT = os.getenv('TG_MONITORING_SUPABASE_STRICT', '1') == '1'\n",
    "SUPABASE_CONSUMER = os.getenv('TG_MONITORING_CONSUMER', 'kaggle')\n",
    "SUPABASE_ACCOUNT = os.getenv('GOOGLE_API_LOCALNAME')\n",
    "SUPABASE_TPM_RESERVE_EXTRA = int(os.getenv('TG_GEMMA_TPM_RESERVE_EXTRA', '1000'))\n",
    "SUPABASE_MAX_RETRIES = int(os.getenv('TG_GEMMA_RETRIES', '2'))\n",
    "\n",
    "\n",
    "def _normalize_model_id(value: str) -> str:\n",
    "    v = (value or '').strip()\n",
    "    if v.startswith('models/'):\n",
    "        v = v[len('models/') :]\n",
    "    if v.startswith('gemma-') and v.endswith('-it'):\n",
    "        v = v[: -len('-it')]\n",
    "    return v or 'gemma-3-27b'\n",
    "\n",
    "\n",
    "def _supabase_rpc(fn_name: str, payload: dict):\n",
    "    if not SUPABASE_ENABLED:\n",
    "        return None\n",
    "    url = SUPABASE_URL.rstrip('/') + f\"/rest/v1/rpc/{fn_name}\"\n",
    "    headers = {\n",
    "        'apikey': SUPABASE_KEY,\n",
    "        'Authorization': f'Bearer {SUPABASE_KEY}',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept-Profile': SUPABASE_SCHEMA,\n",
    "        'Content-Profile': SUPABASE_SCHEMA,\n",
    "    }\n",
    "    resp = requests.post(url, headers=headers, json=payload, timeout=20)\n",
    "    if resp.status_code >= 400:\n",
    "        if SUPABASE_STRICT:\n",
    "            raise RuntimeError(f\"Supabase RPC {fn_name} failed: {resp.status_code} {resp.text}\")\n",
    "        logger.warning('supabase rpc failed: %s %s', resp.status_code, resp.text[:200])\n",
    "        if resp.status_code == 404 and 'PGRST202' in resp.text:\n",
    "            return {'fallback_local': True}\n",
    "        return None\n",
    "    data = resp.json()\n",
    "    if isinstance(data, list) and data:\n",
    "        return data[0]\n",
    "    return data or {}\n",
    "\n",
    "\n",
    "class SupabaseLimiter:\n",
    "    def __init__(self):\n",
    "        self.enabled = SUPABASE_ENABLED\n",
    "\n",
    "    def reserve(self, request_uid: str, attempt_no: int, model_id: str, reserved_tpm: int):\n",
    "        if not self.enabled:\n",
    "            return {'ok': True, 'env_var_name': 'GOOGLE_API_KEY'}\n",
    "        payload = {\n",
    "            'p_request_uid': request_uid,\n",
    "            'p_attempt_no': attempt_no,\n",
    "            'p_consumer': SUPABASE_CONSUMER,\n",
    "            'p_account_name': SUPABASE_ACCOUNT,\n",
    "            'p_model': model_id,\n",
    "            'p_reserved_tpm': reserved_tpm,\n",
    "            'p_candidate_key_ids': None,\n",
    "        }\n",
    "        result = _supabase_rpc('google_ai_reserve', payload)\n",
    "        if isinstance(result, dict) and result.get('fallback_local'):\n",
    "            return {'ok': True, 'fallback_local': True, 'env_var_name': 'GOOGLE_API_KEY'}\n",
    "        return result or {'ok': False}\n",
    "\n",
    "    def mark_sent(self, request_uid: str, attempt_no: int):\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        _supabase_rpc('google_ai_mark_sent', {\n",
    "            'p_request_uid': request_uid,\n",
    "            'p_attempt_no': attempt_no,\n",
    "        })\n",
    "\n",
    "    def finalize(self, request_uid: str, attempt_no: int, *, usage=None, error=None, duration_ms: int | None = None, model_id: str | None = None):\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        payload = {\n",
    "            'p_request_uid': request_uid,\n",
    "            'p_attempt_no': attempt_no,\n",
    "            'p_usage_input_tokens': None,\n",
    "            'p_usage_output_tokens': None,\n",
    "            'p_usage_total_tokens': None,\n",
    "            'p_duration_ms': duration_ms,\n",
    "            'p_provider_status': 'succeeded' if error is None else 'failed',\n",
    "            'p_error_type': None,\n",
    "            'p_error_code': None,\n",
    "            'p_error_message': None,\n",
    "        }\n",
    "        if usage:\n",
    "            payload['p_usage_input_tokens'] = usage.get('input_tokens')\n",
    "            payload['p_usage_output_tokens'] = usage.get('output_tokens')\n",
    "            payload['p_usage_total_tokens'] = usage.get('total_tokens')\n",
    "        if error is not None:\n",
    "            payload['p_error_type'] = type(error).__name__\n",
    "            payload['p_error_message'] = str(error)[:500]\n",
    "        result = _supabase_rpc('google_ai_finalize', payload)\n",
    "        if isinstance(result, dict) and result.get('fallback_local'):\n",
    "            legacy_payload = {\n",
    "                'p_request_uid': request_uid,\n",
    "                'p_api_key_id': None,\n",
    "                'p_model': model_id,\n",
    "                'p_actual_input_tokens': payload.get('p_usage_input_tokens'),\n",
    "                'p_actual_output_tokens': payload.get('p_usage_output_tokens'),\n",
    "                'p_status': 'success' if error is None else 'failed',\n",
    "            }\n",
    "            _supabase_rpc('finalize_google_ai_usage', legacy_payload)\n",
    "\n",
    "async def _call_model(kind: str, prompt: str, images=None):\n",
    "    model_state = MODEL_REGISTRY[kind]\n",
    "    model = model_state['model']\n",
    "    fallback_name = model_state.get('fallback')\n",
    "    has_images = bool(images)\n",
    "\n",
    "    def _gen_config(use_json: bool):\n",
    "        cfg = {\n",
    "            'temperature': 0,\n",
    "            'max_output_tokens': 800,\n",
    "        }\n",
    "        if use_json:\n",
    "            cfg['response_mime_type'] = 'application/json'\n",
    "        return cfg\n",
    "\n",
    "    async def _generate(use_json: bool):\n",
    "        return model.generate_content(\n",
    "            prompt if not images else [prompt, *images],\n",
    "            generation_config=_gen_config(use_json),\n",
    "        )\n",
    "\n",
    "    async def _do_call():\n",
    "        try:\n",
    "            return await _generate(True)\n",
    "        except Exception as exc:\n",
    "            msg = str(exc).lower()\n",
    "            if 'json mode' in msg or 'response_mime_type' in msg:\n",
    "                logger.warning('json mode disabled for model=%s', model_state['name'])\n",
    "                return await _generate(False)\n",
    "            if fallback_name and _is_not_found(exc) and fallback_name != model_state['name']:\n",
    "                logger.warning('model not found, fallback to %s', fallback_name)\n",
    "                _set_model(kind, fallback_name)\n",
    "                return await MODEL_REGISTRY[kind]['model'].generate_content(\n",
    "                    prompt if not images else [prompt, *images],\n",
    "                    generation_config=_gen_config(True),\n",
    "                )\n",
    "            raise\n",
    "\n",
    "    use_supabase = supabase_limiter.enabled\n",
    "    if use_supabase:\n",
    "        request_uid = uuid.uuid4().hex\n",
    "        model_id = _normalize_model_id(model_state['name'])\n",
    "        reserved_tpm = 800 + SUPABASE_TPM_RESERVE_EXTRA\n",
    "        for attempt in range(1, SUPABASE_MAX_RETRIES + 1):\n",
    "            reserve = supabase_limiter.reserve(request_uid, attempt, model_id, reserved_tpm)\n",
    "            if reserve.get('fallback_local'):\n",
    "                supabase_limiter.enabled = False\n",
    "                use_supabase = False\n",
    "                break\n",
    "            if not reserve or not reserve.get('ok'):\n",
    "                blocked = reserve.get('blocked_reason') if reserve else 'unknown'\n",
    "                raise RuntimeError(f'Supabase rate limit blocked: {blocked}')\n",
    "            env_var = reserve.get('env_var_name') or 'GOOGLE_API_KEY'\n",
    "            api_key = os.getenv(env_var) or os.getenv('GOOGLE_API_KEY')\n",
    "            if not api_key:\n",
    "                raise RuntimeError(f'Missing API key for {env_var}')\n",
    "            genai.configure(api_key=api_key)\n",
    "            supabase_limiter.mark_sent(request_uid, attempt)\n",
    "            start = time.time()\n",
    "            try:\n",
    "                resp = await _do_call()\n",
    "                usage_meta = getattr(resp, 'usage_metadata', None)\n",
    "                usage = None\n",
    "                if usage_meta is not None:\n",
    "                    usage = {\n",
    "                        'input_tokens': getattr(usage_meta, 'prompt_token_count', None),\n",
    "                        'output_tokens': getattr(usage_meta, 'candidates_token_count', None),\n",
    "                        'total_tokens': getattr(usage_meta, 'total_token_count', None),\n",
    "                    }\n",
    "                supabase_limiter.finalize(request_uid, attempt, usage=usage, duration_ms=int((time.time() - start) * 1000), model_id=model_id)\n",
    "                return resp\n",
    "            except Exception as exc:\n",
    "                supabase_limiter.finalize(request_uid, attempt, error=exc, duration_ms=int((time.time() - start) * 1000), model_id=model_id)\n",
    "                msg = str(exc).lower()\n",
    "                retryable = any(x in msg for x in ['timeout', 'connection', 'temporary', '503', '502', '504'])\n",
    "                if retryable and attempt < SUPABASE_MAX_RETRIES:\n",
    "                    await asyncio.sleep(0.4 * attempt)\n",
    "                    continue\n",
    "                raise\n",
    "\n",
    "    if use_supabase:\n",
    "        raise RuntimeError('Supabase limiter enabled but no call succeeded')\n",
    "\n",
    "    async with rate_limiter.acquire(_estimate_tokens(prompt, has_images=has_images)):\n",
    "        return await _do_call()\n",
    "\n",
    "\n",
    "def _compute_hash(image_bytes: bytes) -> str:\n",
    "    return hashlib.sha256(image_bytes).hexdigest()\n",
    "\n",
    "\n",
    "def _compute_phash(image_bytes: bytes) -> str | None:\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        return str(imagehash.phash(img))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def upload_to_catbox(image_bytes: bytes) -> str | None:\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            'https://catbox.moe/user/api.php',\n",
    "            data={'reqtype': 'fileupload'},\n",
    "            files={'fileToUpload': ('image.jpg', image_bytes)},\n",
    "            timeout=30,\n",
    "        )\n",
    "        if resp.status_code == 200:\n",
    "            return resp.text.strip()\n",
    "    except Exception as exc:\n",
    "        logger.warning('catbox upload failed: %s', exc)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _message_date_iso(msg):\n",
    "    dt = msg.date\n",
    "    if dt and dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc).isoformat() if dt else None\n",
    "\n",
    "\n",
    "def _message_likes(msg) -> int | None:\n",
    "    reactions = getattr(msg, 'reactions', None)\n",
    "    if not reactions or not getattr(reactions, 'results', None):\n",
    "        return None\n",
    "    return sum(r.count for r in reactions.results if getattr(r, 'count', None))\n",
    "\n",
    "\n",
    "def _source_type(entity) -> str:\n",
    "    if isinstance(entity, Channel):\n",
    "        return 'channel' if getattr(entity, 'broadcast', False) else 'supergroup'\n",
    "    if isinstance(entity, Chat):\n",
    "        return 'group'\n",
    "    return 'unknown'\n",
    "\n",
    "\n",
    "async def extract_events(text: str, ocr_text: str | None = None):\n",
    "    content = (text or '').strip()\n",
    "    if ocr_text:\n",
    "        content = (content + '\\n\\nOCR:\\n' + ocr_text).strip()\n",
    "    if not content or len(content) < 10:\n",
    "        return []\n",
    "    prompt = (\n",
    "        'You extract events from a Telegram message. '\n",
    "        'Return strict JSON array of event objects. '\n",
    "        'If there are no events, return [] only. '\n",
    "        'Fields per event: title, date (YYYY-MM-DD), time (HH:MM or empty), '\n",
    "        'end_date (YYYY-MM-DD or null), location_name, location_address, city, '\n",
    "        'ticket_link, ticket_price_min, ticket_price_max, ticket_status, raw_excerpt, '\n",
    "        'event_type, emoji, is_free, pushkin_card, search_digest, festival. '\n",
    "        'Use null for unknown fields. '\n",
    "        'Message text:\\n' + content\n",
    "    )\n",
    "    try:\n",
    "        resp = await _call_model('text', prompt)\n",
    "    except Exception as exc:\n",
    "        logger.warning('extract_events failed: %s', exc)\n",
    "        return []\n",
    "    data = _safe_json(getattr(resp, 'text', '') if resp else '')\n",
    "    if data is None:\n",
    "        fix_prompt = (\n",
    "            'Fix and return valid JSON only. '\n",
    "            'Do not include any extra text. '\n",
    "            'Input:\\n' + (getattr(resp, 'text', '') if resp else '')\n",
    "        )\n",
    "        try:\n",
    "            resp_fix = await _call_model('text', fix_prompt)\n",
    "            data = _safe_json(getattr(resp_fix, 'text', '') if resp_fix else '')\n",
    "        except Exception as exc:\n",
    "            logger.warning('extract_events json_fix failed: %s', exc)\n",
    "    if isinstance(data, dict) and isinstance(data.get('events'), list):\n",
    "        return data['events']\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    return []\n",
    "\n",
    "\n",
    "async def ocr_image(image_bytes: bytes):\n",
    "    if not ENABLE_OCR:\n",
    "        return None, None\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "    except Exception:\n",
    "        return None, None\n",
    "    prompt = (\n",
    "        'Extract readable text from the image. '\n",
    "        'Return JSON: {\"text\": \"...\", \"title\": \"...\"}. '\n",
    "        'If no text, return {\"text\": \"\"}. '\n",
    "    )\n",
    "    try:\n",
    "        resp = await _call_model('vision', prompt, images=[img])\n",
    "    except Exception as exc:\n",
    "        logger.warning('ocr_image failed: %s', exc)\n",
    "        return None, None\n",
    "    data = _safe_json(getattr(resp, 'text', '') if resp else '')\n",
    "    if data is None:\n",
    "        fix_prompt = (\n",
    "            'Fix and return valid JSON only. '\n",
    "            'Do not include any extra text. '\n",
    "            'Input:\\n' + (getattr(resp, 'text', '') if resp else '')\n",
    "        )\n",
    "        try:\n",
    "            resp_fix = await _call_model('vision', fix_prompt, images=[img])\n",
    "            data = _safe_json(getattr(resp_fix, 'text', '') if resp_fix else '')\n",
    "        except Exception as exc:\n",
    "            logger.warning('ocr_image json_fix failed: %s', exc)\n",
    "    if isinstance(data, dict):\n",
    "        text = data.get('text') or ''\n",
    "        title = data.get('title') or None\n",
    "        if text and not title:\n",
    "            title = text.split('\\n', 1)[0].strip() if text else None\n",
    "        return text or None, title\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scan_source(client: TelegramClient, source: dict) -> list[dict]:\n",
    "    username = (source.get('username') or '').strip()\n",
    "    if not username:\n",
    "        logger.warning('source.skip reason=missing_username')\n",
    "        return []\n",
    "    entity = await client.get_entity(username)\n",
    "    s_type = _source_type(entity)\n",
    "    last_id = source.get('last_scanned_message_id') or 0\n",
    "    default_location = source.get('default_location')\n",
    "    default_ticket_link = source.get('default_ticket_link')\n",
    "\n",
    "    cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_DAYS_BACK)\n",
    "\n",
    "    latest_id = None\n",
    "    latest_date = None\n",
    "    try:\n",
    "        latest = await client.get_messages(entity, limit=1)\n",
    "        if latest:\n",
    "            latest_msg = latest[0]\n",
    "            latest_id = latest_msg.id\n",
    "            latest_date = _message_date_iso(latest_msg)\n",
    "    except Exception as exc:\n",
    "        logger.warning('source.latest_failed %s: %s', username, exc)\n",
    "\n",
    "    logger.info(\n",
    "        'source.start username=%s type=%s last_id=%s latest_id=%s latest_date=%s cutoff=%s',\n",
    "        username,\n",
    "        s_type,\n",
    "        last_id or 0,\n",
    "        latest_id,\n",
    "        latest_date,\n",
    "        cutoff.isoformat(),\n",
    "    )\n",
    "    if last_id and latest_id and latest_id <= last_id:\n",
    "        logger.info(\n",
    "            'source.skip reason=no_new_messages username=%s last_id=%s latest_id=%s',\n",
    "            username,\n",
    "            last_id,\n",
    "            latest_id,\n",
    "        )\n",
    "        return []\n",
    "\n",
    "    messages_out = []\n",
    "    views_vals = []\n",
    "    likes_vals = []\n",
    "    processed = 0\n",
    "    messages_with_events = 0\n",
    "    events_total = 0\n",
    "    first_id = None\n",
    "    last_id_seen = None\n",
    "    first_date = None\n",
    "    last_date = None\n",
    "    cutoff_hit = False\n",
    "\n",
    "    async for msg in client.iter_messages(entity, limit=MAX_MESSAGES_PER_SOURCE, min_id=last_id or 0):\n",
    "        if not last_id and msg.date and msg.date.replace(tzinfo=timezone.utc) < cutoff:\n",
    "            cutoff_hit = True\n",
    "            break\n",
    "        if first_id is None:\n",
    "            first_id = msg.id\n",
    "            first_date = _message_date_iso(msg)\n",
    "        last_id_seen = msg.id\n",
    "        last_date = _message_date_iso(msg)\n",
    "\n",
    "        text = msg.message or ''\n",
    "        views = getattr(msg, 'views', None)\n",
    "        likes = _message_likes(msg)\n",
    "        if isinstance(views, int):\n",
    "            views_vals.append(views)\n",
    "        if isinstance(likes, int):\n",
    "            likes_vals.append(likes)\n",
    "\n",
    "        events = await extract_events(text)\n",
    "\n",
    "        posters = []\n",
    "        ocr_probe_text = None\n",
    "        if not events and msg.photo:\n",
    "            try:\n",
    "                probe_bytes = await client.download_media(msg, bytes)\n",
    "                if probe_bytes:\n",
    "                    ocr_probe_text, _ = await ocr_image(probe_bytes)\n",
    "                    if ocr_probe_text:\n",
    "                        events = await extract_events(text, ocr_probe_text)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if events and msg.photo:\n",
    "            try:\n",
    "                image_bytes = await client.download_media(msg, bytes)\n",
    "                if image_bytes:\n",
    "                    sha = _compute_hash(image_bytes)\n",
    "                    phash = _compute_phash(image_bytes)\n",
    "                    catbox_url = upload_to_catbox(image_bytes)\n",
    "                    ocr_text, ocr_title = await ocr_image(image_bytes) if not ocr_probe_text else (ocr_probe_text, None)\n",
    "                    posters.append({\n",
    "                        'catbox_url': catbox_url,\n",
    "                        'sha256': sha,\n",
    "                        'phash': phash,\n",
    "                        'ocr_text': ocr_text,\n",
    "                        'ocr_title': ocr_title,\n",
    "                    })\n",
    "            except Exception as exc:\n",
    "                logger.warning('media download failed for %s/%s: %s', username, msg.id, exc)\n",
    "\n",
    "        cleaned_events = []\n",
    "        for ev in events or []:\n",
    "            if not isinstance(ev, dict):\n",
    "                continue\n",
    "            if default_location and not ev.get('location_name'):\n",
    "                ev['location_name'] = default_location\n",
    "            if default_ticket_link and not ev.get('ticket_link'):\n",
    "                ev['ticket_link'] = default_ticket_link\n",
    "            cleaned_events.append(ev)\n",
    "\n",
    "        if cleaned_events:\n",
    "            messages_with_events += 1\n",
    "            events_total += len(cleaned_events)\n",
    "\n",
    "        messages_out.append({\n",
    "            'source_username': username,\n",
    "            'source_type': s_type,\n",
    "            'source_chat_id': getattr(entity, 'id', None),\n",
    "            'message_id': msg.id,\n",
    "            'message_date': _message_date_iso(msg),\n",
    "            'source_link': f'https://t.me/{username}/{msg.id}',\n",
    "            'text': text,\n",
    "            'metrics': {\n",
    "                'views': views,\n",
    "                'likes': likes,\n",
    "            },\n",
    "            'posters': posters,\n",
    "            'events': cleaned_events,\n",
    "        })\n",
    "\n",
    "        processed += 1\n",
    "        await human_sleep(HUMAN_DELAY_MIN, HUMAN_DELAY_MAX)\n",
    "        if HUMAN_LONG_PAUSE_EVERY > 0 and processed % HUMAN_LONG_PAUSE_EVERY == 0:\n",
    "            await human_sleep(HUMAN_LONG_PAUSE_MIN, HUMAN_LONG_PAUSE_MAX)\n",
    "\n",
    "    median_views = int(statistics.median(views_vals)) if views_vals else None\n",
    "    median_likes = int(statistics.median(likes_vals)) if likes_vals else None\n",
    "    for msg in messages_out:\n",
    "        msg['metrics']['channel_median_views'] = median_views\n",
    "        msg['metrics']['channel_median_likes'] = median_likes\n",
    "\n",
    "    if not messages_out:\n",
    "        logger.info(\n",
    "            'source.empty username=%s last_id=%s latest_id=%s cutoff_hit=%s',\n",
    "            username,\n",
    "            last_id or 0,\n",
    "            latest_id,\n",
    "            cutoff_hit,\n",
    "        )\n",
    "\n",
    "    logger.info(\n",
    "        'source.done username=%s messages=%d processed=%d messages_with_events=%d events=%d first_id=%s last_id=%s cutoff_hit=%s',\n",
    "        username,\n",
    "        len(messages_out),\n",
    "        processed,\n",
    "        messages_with_events,\n",
    "        events_total,\n",
    "        first_id,\n",
    "        last_id_seen,\n",
    "        cutoff_hit,\n",
    "    )\n",
    "    if first_date or last_date:\n",
    "        logger.info(\n",
    "            'source.dates username=%s first_date=%s last_date=%s',\n",
    "            username,\n",
    "            first_date,\n",
    "            last_date,\n",
    "        )\n",
    "    return messages_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    sources = config.get('sources') or []\n",
    "    run_id = config.get('run_id') or f'kaggle_{uuid.uuid4().hex[:8]}'\n",
    "    all_messages = []\n",
    "\n",
    "    logger.info('tg_monitor.run start run_id=%s sources=%d', run_id, len(sources))\n",
    "    if not sources:\n",
    "        logger.warning('tg_monitor.run no sources configured')\n",
    "\n",
    "    device_config = {\n",
    "        'device_model': 'Samsung S22 Ultra',\n",
    "        'system_version': '13.0',\n",
    "        'app_version': '9.6.6',\n",
    "    }\n",
    "\n",
    "    async with TelegramClient(StringSession(TG_SESSION), int(TG_API_ID), TG_API_HASH, **device_config) as client:\n",
    "        for source in sources:\n",
    "            try:\n",
    "                await human_sleep(SOURCE_PAUSE_MIN, SOURCE_PAUSE_MAX)\n",
    "                msgs = await scan_source(client, source)\n",
    "                all_messages.extend(msgs)\n",
    "                logger.info('scanned %s messages for %s', len(msgs), source.get('username'))\n",
    "            except Exception as exc:\n",
    "                logger.exception('scan failed for %s: %s', source.get('username'), exc)\n",
    "            await human_sleep(SOURCE_PAUSE_MIN, SOURCE_PAUSE_MAX)\n",
    "\n",
    "    messages_with_events = sum(1 for m in all_messages if m.get('events'))\n",
    "    events_extracted = sum(len(m.get('events') or []) for m in all_messages)\n",
    "\n",
    "    logger.info(\n",
    "        'tg_monitor.run summary run_id=%s messages=%d messages_with_events=%d events=%d',\n",
    "        run_id,\n",
    "        len(all_messages),\n",
    "        messages_with_events,\n",
    "        events_extracted,\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        'schema_version': 1,\n",
    "        'run_id': run_id,\n",
    "        'generated_at': datetime.now(timezone.utc).isoformat(),\n",
    "        'messages': all_messages,\n",
    "        'stats': {\n",
    "            'sources_total': len(sources),\n",
    "            'messages_scanned': len(all_messages),\n",
    "            'messages_with_events': messages_with_events,\n",
    "            'events_extracted': events_extracted,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    out_path = Path('telegram_results.json')\n",
    "    out_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "    logger.info('Saved telegram_results.json with %s messages', len(all_messages))\n",
    "\n",
    "try:\n",
    "    _loop = asyncio.get_running_loop()\n",
    "except RuntimeError:\n",
    "    asyncio.run(main())\n",
    "else:\n",
    "    await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}