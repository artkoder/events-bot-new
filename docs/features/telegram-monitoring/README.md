# Интеллектуальный мониторинг ТГ-каналов (Kaggle + Telethon + Gemma)

## 1. Архитектура и Структура

Логика реализуется в пакете `source_parsing/telegram` и новом Kaggle-ноутбуке.
**Основной принцип:** Kaggle — это интеллектуальный фильтр. Он не просто парсит, он принимает решение о новизне события, сверяясь с "картиной мира" (Telegraph-страницами).

## 2. Безопасность и Сессия (Samsung S22 Ultra)

### Генератор сессии
Скрипт `scripts/generate_tg_session.py` предназначен для локального запуска.

### Параметры устройства
Строго зашиты для маскировки под реальное устройство:
- `app_version`
- `system_version`
- `device_model="Samsung S22 Ultra"`

### Передача
Session String передается в Kaggle через механизм **secrets** (разбиение на чанки/датасеты, сборка в памяти), чтобы избежать утечки в публичные логи.

## 3. Логика работы Kaggle Ноутбука (The Brain)

Ноутбук выполняет полный цикл анализа.
**Входные параметры:** `TG_SESSION`, список каналов, ссылки на текущие страницы-афиши месяца в Telegraph (для дедупликации).

### Шаг 1: Сбор и первичная фильтрация (Telethon)
- Имитация поведения человека (human-like requests, случайные задержки).
- **Глубина сканирования (Human-like):**
    - Лимит: Последние **50 постов** или посты за **3 суток** (что наступит раньше).
    - *Почему:* Эмуляция поведения пользователя, который пролистывает ленту за выходные. Глубокий скроллинг (тысячи постов) вызывает подозрения у Telegram.
- **Сбор статистики:** для каждого поста сохраняются `views` и `likes` (если доступно).
- **Анализ канала:** При чтении ленты вычисляются `median_views` и `median_likes` для этого канала (на основе собранной выборки постов) для последующего скоринга.
- **OCR (Распознавание афиш):**
    - **Когда:** Если у поста есть картинка, но мало текста (< 100 символов).
    - **Порядок:** OCR выполняется **ДО** Pre-filtering, чтобы эвристика могла найти дату на картинке.
    - **Технология (Kaggle Specific):** Внутри ноутбука используется **Gemma-3-27b-it** (via Google API).
        - *Обоснование:* Эта модель уже протестирована нами для задач OCR/VQA и является оптимальным выбором (бесплатно в рамках лимитов Google AI Studio).
        - *Важно:* Не использовать OpenAI (`poster_ocr`) в ноутбуке.
    - **Порядок:** OCR выполняется **ДО** Pre-filtering, чтобы эвристика могла найти дату на картинке.
    - Результат OCR добавляется к тексту поста для анализа.
- **Pre-filtering (Gatekeeper):**
    - Эвристическая проверка для экономии токенов Gemma.
    - **Date Mandatory (Строгое правило даты):**
        - Пост **обязан** содержать упоминание даты или времени (число.месяц, день недели, "сегодня/завтра", время ЧЧ:ММ).
        - *Обоснование:* Пост без даты с вероятностью 99% не является анонсом будущего события, который мы можем обработать.
    - **Мягкая фильтрация по ключевым словам:**
        - Если есть дата, наличие слов типа "концерт" *желательно*, но не строго обязательно (чтобы не пропустить киновечер или лекцию с нестандартным описанием).
        - **Fail:** Если нет ни даты, ни времени. ИЛИ если только стоп-слова.
    - **Цель:** Отсеять посты "Доброе утро", "Мы открылись", новости без привязки к событию.

### Шаг 2: Выявление события (Gemma Analysis)
Каждый потенциальный пост отправляется в Gemma.

**Промпт:**
> "Проанализируй текст. Является ли это анонсом события? Если да, извлеки: Название, Дата, Время, Место, Краткое описание. Если нет — верни null".

**Результат:** Структурированный объект события или пропуск.

### Шаг 3: Умная Дедупликация (Gemma + Telegraph Context)
> **Важное изменение:** Дедупликация происходит здесь, до загрузки картинок.

1.  **Загрузка контекста:** Ноутбук парсит текст с актуальных страниц Telegraph (например, "Афиша Январь", "Афиша Февраль"), ссылки на которые переданы при запуске.
2.  **Сравнение:** Gemma получает на вход:
    - Список событий с Telegraph (Название, Дата, Место).
    - Новое найденное событие.
3.  **Промпт:**
    > "Сравни новое событие с существующими. Проверь совпадение по смыслу (нечеткое сравнение названий), дате и локации. Если это событие уже есть в списке — пометь как ДУБЛЬ".
4.  **Действие:** Если дубль — пост отбрасывается (логгируется как `SKIP`).

### Шаг 4: Масштабируемость (Smart Batching)
При большом количестве каналов (>50) последовательный обход выглядит подозрительно.
**Решение:**
- **Порционный запуск:** Скрипт запускается каждый час (или чаще), но за один прогон обрабатывает только **случайную выборку** из 10-15 каналов (или по Round-Robin очереди).
- **Результат:** За сутки каждый канал проверяется минимум 3-4 раза.
- **Эмуляция:** Это имитирует пользователя, который зашел проверить конкретную "папку" в Telegram или ответил на уведомления, а не читает все подписки подряд.

### Шаг 5: Обработка медиа (Catbox)
Выполняется **только если событие прошло проверку на уникальность**.


Если в посте есть изображение (афиша):
1.  Скачать файл в память (или временную директорию Kaggle).
2.  Загрузить на `Catbox.moe`.
3.  Получить URL.
4.  Удалить локальный файл.

### Шаг 5: Формирование результата
Ноутбук сохраняет `telegram_results.json`:

```json
[
  {
    "status": "new",
    "source_link": "https://t.me/channel/123",
    "original_text": "...",
    "catbox_url": "https://files.catbox.moe/xyz.jpg",
    "extracted_data": {
      "title": "Концерт Баха",
      "date": "2025-11-20",
      "time": "19:00",
      "location": "Кафедральный собор"
    },
    "metrics": {
      "views": 1500,
      "likes": 45,
      "channel_median_views": 1000,
      "channel_median_likes": 20
    }
  }
]
```

## 4. Серверная часть (Controller & Writer)

### 4.1. Подготовка запуска (Job Payload)
При запуске Kaggle-ноутбука сервер должен передать ему не только секреты, но и контекст для дедупликации:
1.  Получить из БД актуальные ссылки на Telegraph-страницы месяцев (текущий + следующий).
2.  Передать эти ссылки в конфигурацию ноутбука.

### 4.2. Обработка результата (Gemma Rewrite & Save)
После завершения работы ноутбука сервер забирает JSON.

#### Рерайт (Gemma Only)
Для каждого события из JSON запускается Gemma.
- **Задача:** Переписать `original_text` (или использовать `extracted_data` как фактуру) в стиле "журналиста-событийщика".
- **Очистка:** При рерайте или сохранении **удаляются все хештеги** из исходного текста (п. 1 требований).
- **Запрет:** Использование других LLM (OpenAI и т.д.) запрещено для этой задачи. Только Gemma (через существующий единый фреймворк лимитов).

#### Сохранение (Standard Pipeline)
Финальный этап сохранения полностью использует существующий в проекте стандартный механизм (аналог обработки `/vk` или ручного добавления):
1.  **Вызов стандартного пайплайна:**
    - Использовать существующие функции (`event_utils`, `main.py` handlers) для валидации и сохранения.
    - Не дублировать логику парсинга/сохранения.
    - *Вход:* Текст (с OCR и рерайтом) + Картинка (URL) + Метаданные (Score, Link).
2.  **Скоринг (Unified Scoring):**
    - Рассчитывается единый балл (Base LLM + Popularity Boost).
    - Если `views` > `channel_median_views`, событие получает бонус к оценке.
    - Подробнее см. `docs/features/event-scoring/README.md`.
3.  **Результат:** Событие появляется в базе, создается Telegraph-страница, обновляются дайджесты — всё работает "из коробки" благодаря стандарту.

100: #### Отчетность
101: В админский чат отправляется отчет:
102: > "Найдено событий: X. Из них новых: Y (добавлены). Дубликатов отсеяно в ноутбуке: Z."
103:
104: **Блок "Популярное":**
105: Отдельно в отчете выделяются события с высоким скоринговым баллом (High Score Events), найденные в этот проход. Указывается причина (например: "Просмотров 5000 при норме 1000").
106:
107: Список добавленных событий выдается в виде списка со ссылками на телеграф страницы каждого события.

## 5. Детали реализации и Лимиты

- **Rate Limiter:** Использовать существующий класс `RateLimiter` с настройкой "50% от лимитов Google" внутри ноутбука для запросов к Gemma.
- **Сервисная архитектура:**
    - `source_parsing/telegram/service.py` — оркестратор.
    - `source_parsing/telegram/deduplication.py` — логика подготовки данных для проверки дублей (на сервере).
- **Модульность:** В самом ноутбуке код должен быть модульным (можно использовать загрузку .py файлов из датасета или src папки, как сделано в `UniversalFestivalParser`).

## 6. Управление и Мониторинг (Management Features)

Требования к администрированию, перенесенные из опыта работы с VK.

### 6.1. Управление каналами (CRUD)
Необходим интерфейс (Telegram-команды) для управления списком отслеживаемых каналов:

1.  **Добавление с настройками (Add with Defaults):**
    *   Возможность задать **дефолтную локацию** (`default_location`) для канала. *Пример: для канала "Драмтеатр" локация всегда "Драмтеатр".*
    *   Возможность задать **дефолтную ссылку** (`default_ticket_link`).
    *   *Зачем:* Gemma получает эти данные как контекст. Если в посте написано "Приходите на концерт", но нет адреса, Gemma подставит дефолтную локацию.
    *   **Проверка дублей:** Система должна запретить добавление канала, который уже есть в базе (по ID или username).
2.  **Редактирование:** Изменение настроек существующего канала.
3.  **Удаление (`/tg_del`):**
    *   Явная команда для удаления канала из мониторинга.
    *   Остановка всех запланированных задач для этого канала.

### 6.2. Мониторинг здоровья (Health Check)
Команда списка каналов (`/tg_channels`) должна отображать расширенную информацию:

*   **Статус:**
    *   **Last Scanned (`checked_at`):** Время последней попытки чтения.
    *   **Last Post Found (`updated_at`):** Время последнего найденного *нового* поста.
*   **Статистика аудитории (для скоринга):**
    *   `Median Views`: Медианное количество просмотров на пост.
    *   `Median Likes`: Медианное количество лайков.
    *   *Зачем:* Чтобы понимать "вес" канала при расчете популярности события.

